# -----------------------------------------------------------------------------
# Instructions:
# -----------------------------------------------------------------------------
## This is a template for your .env file. It is used to configure the various
## aspects of the application.
## 1) Uncomment and specify the variables you'd like to use. Usable variables
## are provided in all caps names, e.g., TEMPERATURE.
## 2) Make sure this file is called .env and present at the root of the project.
## 3) We use the dotenv package to load these variables BUT if these variables
## are set somewhere else in your OS, user, or anything else, then the already
## existing values will be used as we do not override existing values.

# -----------------------------------------------------------------------------
# General app configuration:
# -----------------------------------------------------------------------------

## [Optional]
## The minimum level required for log messages to go through. Accepts debug,
## info, warning, error, or critical. If not set, it defaults to "info".
LOGGING_LEVEL = info


# -----------------------------------------------------------------------------
# Django configuration:
# -----------------------------------------------------------------------------

## [Mandatory]
## Your Django secret key. DO NOT push it to any repo, or any public place.
## This would compromise the whole security of the website. To generate a secret
## key, you can use the following command:
## python -c 'from django.core.management.utils import get_random_secret_key; print(get_random_secret_key())
DJANGO_SECRET_KEY = 'My_Very_Secret_Keeeyyyyy'

## [Mandatory]
## If your Django app runs in debug mode or not. Leave it to 1 if you use the
## command `python manage.py runserver`. Set it to 0 if you use a proper
## web service.
DEBUG = 1

## [Mandatory]
## The IP addresses that the Django app is authorised to use. This is specific
## to your deployment and you should always add the IP address of the machine
## on which the service will run.
ALLOWED_HOSTS = 127.0.0.1, localhost


# -----------------------------------------------------------------------------
# LLM configuration:
# -----------------------------------------------------------------------------

## [Optional] - Defaults to 0 (False) if not set.
## Set this to 1 (True) if you don't want the LLM chat to appear on the website.
LLM_DISABLE = 0

## [Optional] - Defaults to 0.5 if not set.
## The temperature for the LLM (between 0 and 1), modifies how "random" the answer
## will be. A temperature of 0 will result in the same answer for the same input
## every time. 
LLM_TEMPERATURE = 0.5

## [Optional] - Defaults to 500 if not set.
## The maximum number of tokens the LLM will use to answer a question. Tokens are
## not words, so it is not recommended to set this too low as the LLM would have
## no space to answer. Setting it too high may result in the LLM blabbering.
LLM_MAX_TOKENS = 1000

## [Mandatory] (if LLM_DISABLE is 0/False)
## The service provider you'd like to use. Options supported so far are openai,
## deepseek, anthropic, and custom. If set to custom, then LLM_URL should be
## specified to provide the URL of the API to use. If set to any other, then
## LLM_API_KEY and LLM_MODEL should be specified.
LLM_SERVICE_PROVIDER = custom

## [Mandatory] (if LLM_SERVICE_PROVIDER is NOT custom)
## The API key provided to your by your LLM service provider.
## Remember to keep your API key secret, DO NOT push it to any repo, or public
## space or people will use it and consume your credits. See the doc of your LLM
## provider of choice on how to obtain an API key.
# LLM_API_KEY = sk-...

## [Mandatory] (if LLM_SERVICE_PROVIDER is NOT custom)
## The name of the model to be used. It is used by LangChain to automatically
## identify the endpoint to be used.
# LLM_MODEL = gpt-4o

## [Mandatory] (if LLM_SERVICE_PROVIDER is custom)
## Use this variable to specify the URL of the API point you'd like to use
## if you're using your own deployment somewhere.
LLM_URL = http://...


# -----------------------------------------------------------------------------
# RAG system configuration:
# -----------------------------------------------------------------------------

## [Optional] - Defaults to '.' (current folder)
## Folder in which the ChromaDB will be created. It will create a 'chromadb'
## folder inside the provided folder.
CHROMA_DB_FOLDER = .

## [Optional] - Defaults to 'all-MiniLM-L6-v2'
## The embedding model to use for the vector DB. This will be retrieved from
## HuggingFace.
# EMBEDDING_MODEL = all-MiniLM-L6-v2

## [Mandatory] (if LLM_DISABLE is 0/False)
## The folders in which knowledge should be discovered. If multiple folders need
## to be searched, then simply separate them with commas.
KNOWLEDGE_FOLDERS = ANPAHP/data, static/documentation, data

## [Optional] - Defaults to 0 (False).
## If set to 1 (True), the provided knowledge folders will be searched recursively.
KNOWLEDGE_SEARCH_RECURSIVELY = 0

## [Optional] - Defaults to all supported file types. 
## See knowledgebase/file_processors.PROCESSORS_REGISTRY to see the list of 
## supported file types. File types should be provided in the form ".*",
## separated by commas.
#KNOWLEDGE_FILE_TYPES = .csv, .pdf

## [Optional] - Defaults to 500 if not set.
## PDF documents will be chunked into small pieces into the vector DB. You can
## configure here how many tokens chunks should contain. Remember, tokens are not
## characters or words, so choose a value wisely. Also the number of tokens
## considered here may not correspond to the number of tokens used by your LLM
## of choice.
CHUNK_SIZE = 1000

## [Optional] - Defaults to 100 if not set.
## Chunks may overlap to provide better document excerpts context. You can 
## configure here how many tokens of overlap there should be between chunks. 
## Obviously, CHUNK_OVERLAP should be smaller than CHUNK_SIZE. More overlap
## provides more context for chunks. If set to 0, there will be no overlap
## between chunks.
CHUNK_OVERLAP = 200

