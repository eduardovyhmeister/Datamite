name,alternative_names,bsc_subfamilies,short_definition,explanation
Access Cost, "['Bandwidth Cost']", "['Data Valuation Techniques']",The cost (not necessarily the price) incurred for accessing a dataset.," The cost (not necessarily price) incurred for accessing a dataset based on historical access logs and the cloud provider's pricing (if existing). This metric directly refers to the cost incurred when accessing data. Data valuation technique which measures access cost based on historical logs and potential cloud pricing, influenced by bandwidth in geo-distributed systems."
Access Frequency,"['Number of Requests', 'Arrival Rate', 'Access Interval', 'Number of Accesses', 'Usage Over Time']", "['Customer Needs and Satisfaction']", Total count of operations or queries made to a system within a given period.,"Overall demand for platform content, capturing the number of times datasets, services, or features are accessed, irrespective of whether by single or multiple users. Also, total count of operations, transactions, or queries made to a system within a given period."
Accessibility,"['Search', 'Functionality', 'Navigation', 'Link']","['Data Valuation Techniques', 'Data Governance and Compliance', 'Technology and Infrastructure']",Ease with which users can access and utilise data,"Accessibility, in the context of data and information systems, refers to the ease with which users can access and utilise data. It encompasses several key aspects:  Availability: Data should be readily available to authorised users when needed, without unnecessary delays or barriers. Usability: The data should be presented in a format that is easy to understand and navigate, allowing users to efficiently find and use the information they need. Interoperability: Data should be compatible with various systems and applications, enabling seamless integration and exchange of information across different platforms. Inclusivity: Accessibility also considers the needs of diverse user groups, including those with disabilities. This means ensuring that data and information systems are designed to be usable by individuals with varying abilities, such as providing alternative formats or assistive technologies. Security and Permissions: While ensuring accessibility, it is also important to manage access controls and permissions to protect sensitive data from unauthorised access. Open to use / licensing: There is no limit on users to use the data. Format: Follow pre-structured definitions for the data content structure, for example, does the dataset match Berners-Lee?s ?linked data principles?? Overall, accessibility is crucial for maximizing the value of data, as it ensures that users can effectively leverage the information for decision-making, analysis, and other purposes. Measures how accessible the platform is for users in terms of ease of navigation, search, and data retrieval."
Accuracy,"[]", "['Data Quality']","How closely data or results align with the true, intended, or expected values.","Accuracy in the context of scientific analysis is a comprehensive measure of how closely data or results align with the true, intended, or expected values. It encompasses multiple facets that ensure the reliability, applicability, and validity of the information within the specific scientific domain. The accuracy of data or results is not only about correctness but also about their overall suitability for achieving the desired objectives in a given context.  The complexity of the relationship between accuracy and the value gained depends on the type of information and the organisation. Moreover, unless specified, the basic calculation of value is obtained by meeting the required accuracy. Accuracy depends on Range, Consistency, Typicality and Moderation Metrics. Nevertheless, different key elements can also be used to explain accuracy: Precision: Refers to the granularity or level of detail in the data. In science, precision ensures that measurements are not only correct but also detailed enough to provide meaningful insights. For example, reporting a temperature as 37.456�C instead of rounding to 37�C can be crucial in laboratory experiments. Timeliness: Information must reflect the most current state. In scientific research, outdated data can lead to incorrect conclusions, making the timeliness of updates or recalibrations essential.  Relevance: Accuracy is context dependent. Data must be both correct and directly applicable to the research question or hypothesis. For instance, accurate but irrelevant data adds no value to the scientific inquiry.  Completeness: Scientific accuracy requires that no critical data points are missing. Incomplete datasets can lead to skewed or biased results, undermining the validity of findings. Traceability: The provenance of data ensures it can be verified and validated. For science, traceability means being able to track data back to its source, such as instrumentation or observation records, to confirm its authenticity. Tolerance and Range: Recognises that minor inaccuracies or deviations may be acceptable within specified thresholds. Scientific accuracy involves defining these thresholds to minimise the impact on overall validity. Furthermore, in terms of range it specifies limits for acceptance too. Context Dependence:  Accuracy must meet the specific standards and expectations of the scientific discipline. For example, a minor deviation in physics experiments might be acceptable, whereas in clinical trials, even small inaccuracies can have serious consequences. Consistency: Data should be reliable and uniform across various trials or applications. Inconsistencies in repeated experiments can signal issues with data accuracy.  Typicality: Measures whether the data reflects typical or expected conditions. Atypical data may require additional validation to ensure its accuracy. Moderation: Ensures data is not skewed or biased toward extremes. Balanced data contributes to overall scientific accuracy by avoiding distortions."
Adaptability,"['Versatility', 'Flexibility']", "['Operational Efficiency']","Adaptability measures a system?s ability to adjust to environmental changes, like network conditions or user demands, to enable dynamic resource allocation and efficient scalability.","Relates to how well a system can adjust to changes in its environment, such as varying network conditions, changing user needs, or different operating contexts. Adaptable systems make scalability more efficient because they can dynamically adjust their resources and behaviour to accommodate growth."
Adaptability Power Curve (APC), "['Adaptation of Data Centre to Available Renewable Energy (APCren)']", "['Operational Efficiency']",Deviation of a data centre?s power use from its baseline,"Adaptability of a data centre?s power consumption by quantifying the deviation of actual monitored power from the baseline power profile over a given time interval. Similarly, APCren can be seen as a modified version that looks for the renewable energy sources influences."
AEUF, "[]", "['Operational Efficiency']", Assess how effectively a data centre employs its airside economiser system for ?free? cooling.,Assess how effectively a data centre employs its airside economiser system for ?free? cooling.
Age,"['Data Creation', 'Time Index', 'Age of Information']", "['Data Quality']","Refers to how recent or old the data is. The age of data can impact its relevance and accuracy, especially in fast-moving fields like technology or finance.","Refers to how recent or old the data is. The age of data can impact its relevance and accuracy, especially in fast-moving fields like technology or finance."
Airflow Efficiency (AFE),"[]", "['Operational Efficiency']",How efficiently air moves from the supply to the return., "How efficiently air moves from the supply to the return."
Annual Fraction of Data Loss (AFDL),"[]", "['Data Quality']", "Measures the fraction of stored data lost annually, assessing reliability in distributed systems.","Measures the fraction of stored data lost annually, assessing reliability in distributed systems."
Availability,"['Retrievability']", "['Operational Efficiency']", "Availability measures how reliably authorized users can access data when needed, encompassing system reliability, fault tolerance, backups, and redundancy.","Degree to which data is accessible and retrievable when needed, by authorised users or systems. It ensures that the data can be accessed in a timely manner without interruption, and is crucial for maintaining smooth operations, decision-making, and service continuity. As a KPI it could have the following Key Aspects: Accessibility: Ensuring that the data is available to authorised users when they need it, whether for operational or analytical purposes. (i.e. a Boolean per dataset). Reliability (Measured through System Robustness): The data should be consistently available without frequent downtime or failures. Fault tolerance (Measured through System Robustness): Systems storing the data should be resilient to hardware or software failures, ensuring minimal disruption in data access. Backup: Implementing regular backups and quick recovery solutions ensures that data is still available after incidents like data corruption or system failures. Redundancy: Data availability can be enhanced through redundant systems (e.g., having multiple copies of the data across servers), so if one system fails, another is still operational."
"Availability, Capacity, and Efficiency (ACE)","[]", "['Operational Efficiency']", "ACE in HVAC tracks: Availability, Capacity, and Efficiency via general mathematical expressions.","In the context of Heating, Ventilation, and Air Conditioning (HVAC) systems, performance is often evaluated based on metrics such as Availability, Capacity, and Efficiency. General expressions can be used to track ACE."
Backup,"['System Backup', 'Recovery Capabilities', 'Data Redundancy Reduction']", "['Operational Efficiency']","Backup assesses a system?s secure data preservation and rapid recovery capabilities during disruptions, while Data Redundancy Reduction gauges how efficiently unnecessary data copies are eliminated for scalable services.","System?s ability to securely preserve data and ensure its availability for recovery during disruptions, emphasizing robust mechanisms and minimal data loss. The opposite perspective of backup is Data Redundancy Reduction that can be a metric when the objective is a scalable service (big IoT services) without considerable repetition."
Budget,"['Costs Planned', 'IT Plan', 'Plan']", "['Data Monetization']", "A budget outlines projected income and expenditures over a defined period to guide financial planning and goal achievement.", "A budget is a financial plan that outlines expected income and expenditures over a specific period. It represents an estimate of how much money an individual, business, government, or organisation expects to earn (revenue) and how much it plans to spend (expenses) to achieve certain financial or operational goals."
Cache Size,"[]","['Technology and Infrastructure']","Cache capacity indicates the total volume of data a cache can hold (in KB, MB, GB, etc.) to accelerate future data retrieval.","Amount of data (measured in units such as kilobytes, megabytes, gigabytes, etc.) that a cache can store at any given time. The cache is a high-speed data storage layer that stores a subset of data, typically temporary, to serve future requests more quickly than retrieving the data from its primary storage or origin location."
CAPEX,"['Capital Cost', 'Hardware Costs', 'Software/Application Costs', 'Service Cost', 'Infrastructure Unit Costs']", "['Data Monetization']", "Capital expenditure covers one-time, upfront investments in acquiring or upgrading physical assets and infrastructure, such as machinery, buildings, data?capturing hardware, acquisition systems, storage and processing equipment, valued at purchase cost or depreciation, with pre-installed software costs excluded.","One-time, upfront investments made to acquire, upgrade, or maintain physical assets or infrastructure, independent on the number of quotes needed to cover this expense. Examples: (1) Purchasing new machinery or equipment; (2) Constructing buildings or facilities; (3) Upgrading technology infrastructure (e.g., data centres, servers); (4) Acquiring servers or other long-term assets; (5) Investing in property or land.  For a data and information system, it encompasses the following components: (1) Data Capturing Hardware: Computers and devices used to collect and input data; (2) Acquisition Systems: Sensors, data acquisition (DAQ) systems, and similar tools for measuring and capturing values; (3) Information Collection Mediums: Devices like PDAs and tablets used for collecting data. (4) Data Storage Hardware: Systems for storing large volumes of data. (5) Data Processing, viewing, and Analysis Hardware: Computers and servers for processing and analysing data. If existing hardware is repurposed, the cost should be based on its depreciation during use. If fully depreciated, the cost is effectively zero.  Although the cost of software can be significant, other software such as Microsoft Office are typically pre-installed on the computers bought by organisations and used by employees. Therefore, they normally run at zero cost for the node and can excluded from its costs."
Carbon Emission Factor (CEF), "[]", "['Innovation and Growth']", "Coefficient used to calculate the amount of CO2 emissions produced per unit of energy consumed.", "Coefficient used to calculate the amount of CO2 emissions produced per unit of energy consumed, typically expressed in kilograms of CO2 per kilowatt-hour (kgCO2/kWh). It varies depending on the energy source (e.g., coal, natural gas, renewable energy)."
Carbon Usage Effectiveness (CUE), "[]", "['Innovation and Growth']", "Measure the environmental impact by assessing the amount of CO2 emissions per unit of IT energy consumed.", "A carbon metric used to measure the environmental impact by assessing the amount of CO2 emissions per unit of IT energy consumed." 
Churn, "[]", "['Customer Needs and Satisfaction']", "Refers to the loss of customers. Prediction uses demographics, usage, transactions, interactions, sentiment and external factors to anticipate customer loss.","Refers to the loss of customers. Prediction uses demographics, usage, transactions, interactions, sentiment and external factors to anticipate customer loss."
Clarity,"['Understandability', 'Easy to Understand', 'Lack of Confusion', 'Unambiguity', 'Concise', 'Readability', 'Interpretability']","['Customer Needs and Satisfaction']","Data Clarity describes how organized, consistent, readable, and well-contextualized data is, enabling users to interpret and use it without confusion.","Refers to how easily the data can be understood, interpreted, and used by its intended audience. It involves presenting data in a way that is straightforward, unambiguous, and free from unnecessary complexity. Clear data allows users to quickly grasp its meaning, draw insights, and make decisions without confusion or misinterpretation. The Key Aspects of Data Clarity from a KPI perspective could be: Simplicity and Structure: Data should be presented in a well-organised manner, often through tables, graphs, or summaries that make complex data easier to digest. Example: Using clear, concise headers in a dataset and avoiding overly complex structures. Consistency: Consistent terminology, formats, and units of measurement improve clarity. Data should follow the same structure and presentation style across the dataset. Readability: Data should be easily readable, with clear labels, appropriate formatting, and a clean layout. Good readability ensures that the user can navigate the data without confusion. Context, Metadata, and/or Explanation: Providing context or explanations (e.g., metadata, labels, or definitions) that clarify what the data represents. Redundancy/Containment Fraction: Data should be free from unnecessary repetition or irrelevant information that can clutter the understanding (within the same dataset - since redundancy is important for availability). Example: Avoiding the use of the same information in multiple columns or rows that do not add value to analysis. Visualisation: Using the right type of visualisation (charts, graphs, maps, etc.) can enhance clarity by making patterns, trends, or insights easier to recognise. Completeness: checks if the dataset has all the required fields and values without missing information (measured as a percentage). Not all these factors are included in the main taxonomy figure, for simplicity they have been reduced there. Nevertheless, that factors could be considered as a base since several of the main components here can be related to readable, conciseness, and understandability."
CO2 Savings, "[]", "['Operational Efficiency']", "CO2 Savings measures the reduction in a data centre?s carbon emissions relative to a baseline achieved through energy optimizations, renewable integration, and heat recovery.","The metric CO2 Savings in the document refers to the change in data centre CO2 emissions from a given baseline. For example, this metric evaluates the reduction in CO2 emissions achieved by implementing the GENiC system and its integrated management strategies for optimizing energy usage, renewable energy integration, and heat recovery in data centre"
Competitive Advantage, "[]", "['Customer Needs and Satisfaction']", "Degree to which data confers a unique strategic edge; impact gauged by potential consequences if competitor?s access or you lose the data.", "Degree to which data confers a unique strategic edge; impact gauged by potential consequences if competitor?s access or you lose the data."
Completeness, "['Appropriate Amount of data']", "['Data Quality']", "Gauges how fully a dataset contains required information?ensuring mandatory fields are populated, missing values are minimal, structure matches expectations, and granularity meets analysis needs.","Extent to which all required data is available and present within a dataset. It measures how much of the expected information is provided, indicating whether critical data fields are missing or left blank. Key aspects of data completeness include: Presence of mandatory fields: All required fields have values. Proportion of missing data: A low percentage of missing or null values indicates better completeness. Consistency with expectations: The dataset should align with the expected structure and content, covering all data points. Granularity: The data should provide enough detail to meet the desired level of analysis."
Compute Power Efficiency (CPE), "[]", "['Operational Efficiency']", "Metric that assesses the efficiency of IT equipment in a data centre by measuring how much of the total power consumption is effectively used for computation.", "Metric that assesses the efficiency of IT equipment in a data centre by measuring how much of the total power consumption is effectively used for computation."
Conciseness,"['Concise Representation', 'Simplicity']", "['Data Quality']", "Captures how briefly and clearly information is conveyed by minimizing redundancy and noise while preserving essential content.", "Principle of conveying essential information briefly, clearly and efficiently by minimizing redundancy and noise. Key characteristics of Concise Representation could include: Brevity/Compression: The information is presented using the fewest words, symbols, or data points necessary. It also Reflects how much data can be compacted without losing critical information. Clarity: The content is clear and easy to follow, even when reduced to its essential elements. Efficiency/Signal-to-noise Ratio: It avoids redundancy, long-winded explanations, or superfluous data.  Redundancy: Measures the repetition of identical or similar data within a system or dataset. Relevance: Only the most relevant information is included, eliminating any extraneous data that does not add value to the understanding of the content. Semantic Consistency: Measures alignment and consistency in the terminology and format used across the dataset. Duplicate Elimination: A measure for the process of identifying and removing identical records."
Confidence, "[]","['Operational Efficiency', 'Data Quality']",The confidence metric measures how strongly a dependency holds in the dataset.,The confidence metric measures how strongly a dependency holds in the dataset.
Consistency,"['Linked to Heterogeneity', 'Veracity']", "['Data Quality']", "How well data adheres to predefined rules.", "Refers to how well data values conform to predefined rules, such as association rules in the context of relational databases. The metric for consistency evaluates the consistency of a tuple t based on whether it fulfils or violates certain rules r from a set R of association rules. Consistency means that data is uniform and reliable across all datasets, systems and applications, remaining accurate and contradiction-free. Key KPI aspects include integrity, uniformity, synchronisation, rule validation, error prevention and governance."
Containment Fraction, "[]","['Operational Efficiency', 'Data Quality']",How much of one dataset is encompassed by another.,"Measures the extent to which one dataset is contained within another. This metric helps identify redundancy and optimise storage usage. A limit in containment fraction must exist since availability depend on duplication, for security, of identical information."
Cooling Capacity Factor (CCF), "[]", "['Operational Efficiency']", Assess the utilisation efficiency of the cooling infrastructure.,Assess the utilisation efficiency of the cooling infrastructure.
Cooling Effectiveness Rate (CER), "['Energy Effectiveness of Cooling']", "['Operational Efficiency']", "Focuses on the effectiveness of cooling systems in the data centre." ,"Focuses on the effectiveness of cooling systems in the data centre. There is a direct link to other metrics related to cooling effectiveness, such as ?Energy Effectiveness of Cooling Mode in a Season?. Assesses how effectively the cooling system operates in different seasons, which can influence operational costs and carbon emissions."
Corporate Average Data Centre Efficiency (CADE), "[]", "['Operational Efficiency']", "Comprehensive assessment of a corporation?s data centre energy performance by considering both infrastructure efficiency and IT asset utilisation.", "CADE is designed to provide a comprehensive assessment of a corporation?s data centre energy performance by considering both infrastructure efficiency and IT asset utilisation. CADE allows the calculation and measurement of a data centre?s energy consumption so that it can be compared to the other data centres."
Cost of Degradation (CoD), "[]", "['Data Quality']", "Quantifies the loss of data quality due to privacy?preserving transformations.", "This metric quantifies the loss of data quality due to privacy?preserving transformations by measuring the reduction in utility between the original and adjusted data."
Currency, "['Linked to Timeliness']", "['Data Quality']", "Gauges data freshness by comparing its age against its required update frequency","Measures the change in data value given change in time or processes in the data. Currency measures how ?fresh? data is by comparing its age (time since last update) against how often it should be updated, while timeliness measures how quickly data arrives or is available when it?s needed?i.e. the delay between when a value is created and when it?s delivered or used."
Data Acquisition Cost (DAC), "['Cost of Procurement']", "['Data Monetization']", "Fraction of local models or data a federated learner must obtain from providers each training epoch to achieve an accurate global model.","Data Acquisition Cost (DAC) is the fraction of local models or data a Data Acquirer must obtain from Data Providers, each training epoch in federated learning, to build an accurate global model; all other expenses (e.g., purchase price, OPEX) are tracked separately."
Data Centre Compute Efficiency (DCcE), "[]", "['Operational Efficiency']", "Ratio of computational work performed by a data centre?s IT equipment to the energy consumed", "Assess the efficiency of computing resources within a data centre. It provides insights into how effectively the data centre?s IT equipment is utilised to perform computational tasks relative to the energy consumed."
Data Centre Lighting Density (DCLD), "['Lighting Power Density (LPD)']", "['Operational Efficiency']", "Is the lighting power consumption per unit area?expressed in W/ft² or W/m²?to evaluate a data centre?s lighting efficiency.","Measures the electrical power used for lighting per unit area within a data centre, typically expressed in watts per square foot (W/ft²) or watts per square meter (W/m²). This metric helps assess the energy efficiency of the lighting system in the facility."
Data Centre Adaptation (DCA), "['Data Centre Energy Profile Change']", "['Operational Efficiency']", "Measures the change in a data centre?s energy profile from a baseline to assess its efficiency and adaptability.", "The Data Centre Energy Profile Change is referred to as DCA (Data Centre Adaptation) in the context of energy metrics outlined in the GENiC framework from the manuscript. It captures the change in the data centre?s energy profile from a predefined baseline. This metric is part of a broader set of evaluation measures used to assess energy efficiency, sustainability, and the ability to adapt to renewable energy integration."
Data Ingestion Capabilities,"['Data Collection and Management Capabilities', 'Data Analysis', 'Data Mining', 'Data Sources']", "['Technology and Infrastructure']", "Describes a system?s ability to ingest and store high-volume, low-latency data from diverse sources at scale.", "Data Ingestion Capabilities describe a system?s ability to acquire, collect and store data from diverse sources at scale and speed. Key aspects include high throughput, low latency (including delayed?data handling), scalability under growing device or data volumes, and integrity maintenance. Common technologies are time?series databases, message queues (e.g. Kafka) and compressed formats like TsFile."
Data Price,"['Data Value', 'Price Function', 'Payoff', 'Reimbursement', 'Financial Value', 'Price of Information']", "['Data Monetization']", "The financial value assigned to data through market or model mechanisms; covering cost, payoff, reimbursement, and volume-based pricing.", "Data price is the financial value assigned to data, determined through models or market mechanisms. It encompasses concepts like monetary cost, price functions, payoff (reward for contributions), reimbursement (compensation for costs), and volume-based metrics (e.g., price per MB). Methodologies include entropy-based valuation, utility models, and dynamic pricing, tailored to specific contexts such as privacy, real-time trading, or structured markets."
Data Principles and Practices,"['Data Standard Driven', 'Standardisation']", "['Governance and Compliance']", "Measures compliance with defined data standards and governance policies to ensure data consistency, reliability, and strategic alignment.","This metric evaluates how closely an organisation complies with defined data standards and governance policies to ensure data consistency, reliability, and alignment with its objectives. It reflects a commitment to maintaining high-quality data by enforcing predefined standards and measures compliance through clear, quantifiable thresholds in data-quality assessments."
Data Similarity (this is different from purely ?similarity?),"['Euclidean Distance', 'projection similarity', 'similarity score', 'cosine similarity', 'average distance', 'Kolmogorov-Smirnov', 'Mann-Whitney', 'Mood?s Median', 'Levene?s test (LE)']", "['Data Quality']","Distance metrics quantify numerical distances between points rather than relying on syntactic likeness; for example, projection similarity assesses datasets by comparing their feature dimensions.","Distance metrics quantify numerical distances between points rather than relying on syntactic likeness; for example, projection similarity assesses datasets by comparing their feature dimensions."
Data Type, "[]", "['Data Valuation Techniques'], "Classifies data into four types: A (operational), B (one-time decisions), C (legal/safety), and D (research/innovation).","Using decision-based valuation, data is classified into four categories: A for operational?frequent, short-lived, highly time-sensitive; B for one-time decisions?nonrecurring, high-accuracy, long realisation period; C for legal/safety?low immediate value but legally retained long-term; and D for research/innovation?uncertain future value with extended lifespan. Other categories can be used if desired."
Data Value (DV),"['Data Criticality', 'Value of Information', 'Fixed Record Value', 'IP Value', 'Intrinsic Record Value']", "['Data Valuation Techniques']","Context-dependent relevance of data to a specific user or decision, while data criticality measures how indispensable that data is to operations and decision-making.","Do not confuse Price with Value. Data value denotes a data object?s relevance to a specific consumer, shifting with application context and needs even when its quality remains unchanged. In distribution services, it quantifies the worth of a decision node within the Decision-Based Valuation framework by weighing cost and lifecycle. Closely related, data criticality measures how indispensable certain data is to business operations and decision-making, assessing its impact on processes, revenue, and service delivery."
Data Value Ratio (VR), "[]", "['Data Valuation Techniques']", "The Data Value Ratio (VR) represents the proportion of a decision node?s value attributed to a specific data source.", The Data Value Ratio (VR) represents the proportion of a decision node?s value attributed to a specific data source.
Data Centre Performance Efficiency, "['DCPE']", "['Operational Efficiency']", "Metric used to evaluate how effectively a data centre utilises energy to perform useful work.", "Metric used to evaluate how effectively a data centre utilises energy to perform useful work."
Data Centre Performance Per Energy, "['DPPE']", "['Operational Efficiency']", "How efficiently a data centre converts energy into computational performance.", "Measure of how efficiently a data centre converts energy into computational performance."
Data Centre Power Density, "['DCPD']", "['Operational Efficiency']", "Measure how much power is consumed per rack in a data centre." ,"Metric used to measure how much power is consumed per rack in a data centre. It helps data centre operators assess how efficiently they are utilizing their rack space and power capacity."
Data Centre Productivity, "['DCP']", "['Operational Efficiency']", "Useful computational work produced per unit of energy consumed.","Metric that evaluates how effectively a data centre converts its consumed energy into useful computational work. Unlike efficiency metrics like PUE, which focus on energy distribution, DCP emphasizes the actual output or performance of a data centre in relation to its energy consumption in an area or general components."
Data Centre Space Efficiency, "['DCSE']", "['Operational Efficiency']', "Physical space utilised per unit of available space.", "Assesses how effectively a data centre utilises its available physical space. Efficient space utilisation is crucial for optimising operational costs, energy consumption, and overall performance."
Data Centre Workload Power Efficiency, "['DWPE']", "['Operational Efficiency']", "IT equipment energy consumption divided by total data centre energy consumption.", "Bridges the gap between infrastructure-level and workload-level energy efficiency. It is the ratio of the energy consumption of the IT equipment to the energy consumption of the entire data centre."
Data Centre Energy Productivity, "['DCeP']", "['Operational Efficiency']", "Useful work output per unit of energy input.", "The DCeP essentially defines the data centre as a blackbox?power goes into the box, heat comes out, data goes into and out of the black box, and a net amount of useful work is done by the black box. In other words, quantifies useful work compared to the energy it requires. It can be calculated for an individual IT device or a cluster of computing equipment."
Data Robustness, "['Shapley Robust']", "['Operational Efficiency']", "Ability of data to maintain value and usability across tasks.", "Data robustness ensuring that datasets maintain their value and usability across various prediction tasks."
D-T Per Cabinet, "['T - Per Cabinet']", "['Operational Efficiency']", "Temperature difference between cabinet inlet and outlet air.","Measures the temperature difference between the air entering and exiting a server cabinet, reflecting cooling efficiency and equipment health; it is aggregated here with the similar ?Temperature per Cabinet? metric, which also tracks cabinet?level thermal conditions."
Demand,"['Score', 'Number of Data Consumers']", "['Data Monetization']", "Buyers? willingness to pay for data, quantified by desirability scores.","The demand or score for data is defined differently depending on the system under consideration and thus, the strategy to be implemented. For example, for DaaS demand for data is conceptualised as buyers' willingness to pay for specific types of data. This demand is measured using a scoring mechanism where buyers rate the desirability of different categories of data. Also, different models, such as Hotelling, can represent system and demand. Independently, this is a new domain in which no clear estimation of demand has been defined."
Deployed Hardware Utilisation Efficiency, "['DH-UE']", "['Operational Efficiency']", "Ratio of minimum servers required for peak load to total servers deployed.", "Measures the efficiency of deployed servers by comparing the minimum number of servers required to handle peak compute load to the total number of servers deployed."
Detail, "[]", "['Data Quality']", "Level of data detail?its granularity and precision.", "There is no clear specification of Detail for its estimation in reference. Nevertheless, purely technical accuracy considerations could imply how finely data is broken down or how much descriptive information is captured in each data point. Then key aspects of data can incorporate: Granularity: The extent to which data is broken down into smaller, specific components. High granularity means more detailed data. Descriptive Attributes: The number of attributes or fields that describe an entity or event. More detailed datasets have a larger number of attributes that provide rich, descriptive context. Precision: How exact the data is. For numerical data, this might refer to how many decimal points are included, for temporal the description of timestamps, while for categorical data, it refers to the specificity of categories. Other business-related measures can also be considered."
Differential Privacy, "[]", "['Data Governance and Compliance']", "Limits how much an algorithm?s output can change when a single individual?s data is added or removed.", "Same as Inferential Privacy, but in the opposite direction. Also a metric used to gauge the level of privacy preserved in data when shared or sold in marketplaces. Differential privacy provides a rigorous, quantitative way to limit how much information about any one individual (or data point) can be inferred from the output of an algorithm. At a high level, it says: ?If you add or remove a single individual?s data from the dataset, the algorithm?s output distribution shouldn?t change too much?. Differential privacy is a widely adopted concept to quantify the privacy loss in private ML algorithms."
Discoverability, "['Platform Performance']", "['Innovation and Growth']", "Effectiveness of a platform in returning relevant datasets for user queries.", "This metric describes the ability of a dataset to show the content of information and be able to search or discover datasets. For a platform, measures the platform?s ability to meet the user?s information needs accurately. This KPI evaluates how effective the search functionality is in returning relevant datasets based on user input."
Downloads, "['Download Frequency']", "['Customer Needs and Satisfaction']", "The number of times users have clicked to retrieve a dataset.", "The number of times users have clicked to retrieve a dataset."
EAFDL, "[]", "['Data Quality']", "EAFDL measures annual data loss as a fraction of total stored data.", "EAFDL measures annual data loss as a fraction of total stored data."
Ease of Measurement, "[]", "['Data Valuation Techniques']", "Readiness of a data value dimension to be quantified using existing methods and tools.", "Ease of Measurement gauges how readily a data value dimension can be quantified using existing methods, tools or frameworks. It reflects the effort and complexity required to evaluate that dimension?helping you focus on those that are straightforward to assess and flagging ones that may need extra work or context."
Easy-to-Use,"['Easy to Use', 'Ease of Use', 'Search (for queries)', 'User Friendliness']", "['Data Quality']","Ability to enable efficient, effective, and satisfying goal achievement with minimal effort.","The degree to which a system, product, or interface is designed to enable users to efficiently, effectively, and satisfactorily achieve their goals with minimal effort and complexity, ensuring accessibility and user-friendliness in operation and learning."
Economic Efficiency,"['Data Business', 'Characteristic Index']", "['Data Monetization']", "Weights data?s operational relevance to gauge its criticality for business processes.", "This index gauges data criticality for business processes by weighting its operational relevance?higher weights signal greater importance. Economic efficiency ensures that the gains from data quality metrics exceed the costs of applying and improving them."
Elapsed Time, "[]", "['Operational Efficiency']", "Total time taken to complete an operation.", "Total time taken to complete an operation."
Electronics Disposal Efficiency, "['EDE']", "['Operational Efficiency']", Assesses the disposal of decommissioned Information and Communication Technology (ICT) assets.,Assesses the disposal of decommissioned Information and Communication Technology (ICT) assets.
Encryption Time (ET), "['Decryption Time (DT)']", "['Operational Efficiency']", Time required to encrypt data.,Measures the time taken to convert plaintext data into an encrypted format using encryption algorithms. It?s important in cloud and data security to understand how quickly data can be secured before transmission or storage.
Energy Data Centre, "['EDC']", "['Operational Efficiency']", "Total energy consumption of a data centre, encompassing energy usage of both IT hardware and supporting infrastructure.", "Total energy consumption of a data centre, encompassing energy usage of both IT hardware and supporting infrastructure."
Energy Efficiency,"['Power to Performance Effectiveness (PPE)', 'PUE', 'DCiE', 'Data Centre Performance per Energy (DPPE)', 'DCEP, Communication Network Energy Efficiency (CNEE)', 'Network Power Usage Effectiveness (NPUE)', 'Energy Proportionality Coefficient (EPC)']", "['Operational Efficiency']", "Workloads processed per unit of energy consumed.","Measures how efficiently the system handles workloads concerning energy consumption. This ratio is a key metric for operational efficiency. Please, refer to each term in this table for further information. Power usage Effectiveness - PUE compares total facility energy to IT?equipment energy, with Data Centre infrastructure Efficiency - DCiE as its inverse. Japan?s Green IT Council?s Data Centre Energy Productivity - DCPE metric merges GEC, PUE, IT Equipment Energy Efficiency - ITEE and IT Equipment Utilisation - ITEU for an all?around efficiency score. Power to Performance Effectiveness - PPE measures power per performance, DCEP tracks useful work per energy, CNEE gauges energy per data unit, NPUE isolates network?component energy, and EPC assesses how device energy scales with workload."
Energy ExpenseS (EES), "[]", "['Operational Efficiency']", Change in energy expenses relative to a baseline following upgrades or flexibility measures.,Quantify how the Energy ExpenseS have been altered (i.e. increased or decreased) compared to a baseline scenario after the equipment is upgraded or the introduction of flexibility mechanisms.
Energy reuse faction (ERF), "['Energy reuse effectiveness (ERE)']", "['Operational Efficiency']", "Defined in ISO/IEC 30134-6 / EN 50600-4-6, determines the share of the total energy consumption that is reused.", "The ERF, defined in ISO/IEC 30134-6 / EN 50600-4-6, determines the share of the total energy consumption that is reused."
Energy Waste Ratio, "['EWR']", "['Operational Efficiency']", "Proportion of energy in a data centre that does not directly contribute to IT operations.", "Quantifies the proportion of energy in a data centre that does not directly contribute to IT operations."
Entropy,"['Shannon?s Entropy', 'Heterogeneity', 'Information Entropy', 'Additional information Value (AIV)', 'Joint Entropy', 'Individual Entropy', 'Information Score']", "['ata Quality']", "Measures dataset randomness or information content.", "Entropy quantifies a dataset?s randomness or information content?such as spatial and temporal diversity in travel?time predictions. Additional Information Value measures how much entropy reduction a transformation adds. Joint entropy captures the combined uncertainty of two variables, while conditional entropy is the uncertainty remaining in one once the other is known."
Error Rate / Ratio / Count,"['Uplink / Downlink Error Rate', 'Trouble Tickets', 'Inter-Server Error Rate (ISER)', 'Failure Rate']","['Operational Efficiency']", "Proportion of operations that fail.", "Quantifies the proportion of failed operations within a system, network, or process over time. It is a key metric in service reliability, communication networks, machine learning, databases, and cloud computing to assess stability and efficiency."
Extensibility, "[]", "['Innovation and Growth']", Ability to add new features or devices without disrupting existing systems.,"Ability to add new features or devices without disrupting existing systems, linked to scalability."
FAIRness Score, "[]", "['Data Quality']", "Evaluates datasets? compliance with the FAIR principles.,Evaluates datasets? compliance with the FAIR principles. More information can be found at https://www.go-fair.org/fair-principles/.
Field Value, "[]", "['Data Monetization']", "Relative importance of a dataset attribute to a business, model, or system.","Denotes the relative importance of a single dataset attribute to a business, model or system. It can be gauged in several ways, including market - based methods (data prices and benchmarks), utility - based measures (impact on outcomes and usage frequency), cost - based calculations (acquisition, storage and maintenance expenses), content based criteria (uniqueness, completeness and accuracy), risk - based assessments (opportunity cost and loss of information value) and model - based techniques like Shapley values, which quantify each field?s contribution to predictive accuracy."
Fixed to Variable Energy Ratio,"['FVER', 'DC-FVER']", "['Operational Efficiency']", "Assess the proportion of fixed energy consumption relative to variable energy consumption.", "Metric designed to assess the proportion of fixed (constant) energy consumption relative to variable (dynamic) energy consumption."
Format,"['Format Compliance', 'Codification', 'Conformity', 'Available Formats']", "['Operational Efficiency']", "Measures adherence to specified data formats.", "Measures adherence to specified data formats to ensure data structure and quality, often calculated as conformity rate."
Granularity,"['Data Frequency', 'Abundance']", "['Data Quality']", "Degree of detail or precision in data.","Refers to the level of detail or precision of the data being collected, stored, and analysed (or space between time stamps for dynamic data). It indicates how finely data is broken down into its components and can significantly impact the quality and usefulness of the data for various applications."
Green Energy Coefficient, "['GEC']", "['Operational Efficiency']", "Share of a data centre?s total energy consumption sourced from renewables.", "Metric that quantifies the proportion of energy used in a data centre that comes from renewable energy sources. It is used to assess the sustainability and environmental impact of data centre operations."
Grid Utilisation Factor, "['GUF']", "['Operational Efficiency']", "Proportion of power drawn from the electrical grid versus generated on-site.", "Assesses the extent to which a data centre relies on power from the electrical grid versus on-site power generation."
Growth Rate, "[]", "['Innovation and Growth']", "Rate of increase in dataset record count over time.", "Measures the increase in the number of records within a dataset over a specific time. This metric tracks the expansion of data volume and provides insights into data collection and acquisition trends. A higher growth rate indicates enhanced data generation or collection capabilities, while a stable or declining rate may signal data input limitations. It is essential for planning scalability, storage, and future data resource value."
Hop Distance (HD),"['Uplink/Downlink HD (UDHD)', 'InterServer HD (ISHD)']", "['Operational Efficiency']", "Number of intermediate devices data passes through in a network.","Measures the number of intermediate devices data traverses in a network, affecting efficiency and latency."
HVAC System Effectiveness, "['HSE']", "['Operational Efficiency']", Overall efficiency of a data centre?s cooling system.,Helps to measure the overall efficiency of a data centre?s cooling system.
Inclusiveness,"[]", "['Innovation and Growth']", "How well an initiative leverages diverse stakeholder contributions while still achieving its goals.", "Measures how well a strategy or data-driven initiative invites and leverages contributions from a broad spectrum of stakeholders while still meeting its essential goals. It highlights the value of integrating diverse perspectives and inputs to drive greater innovation and value without sacrificing quality or performance. Key benefits include: Diverse Insights and Innovation: Organisations ensure they gather a wide range of viewpoints and expertise, which fuels better decision-making, sparks creative solutions, and delivers stronger outcomes. Enhanced Stakeholder Engagement: A focus on inclusiveness deepens involvement from customers, employees, and partners, building trust, loyalty, and collective ownership of results. Maintaining Quality Amid Participation: Strikes the right balance between broad participation and high performance, ensuring that every contribution adds value rather than detracts from core objectives."
Inferential Privacy, "[]", "['Data Governance and Compliance']", "Probability an adversary correctly infers a private parameter from observed data.","Quantifies the probability that an adversary can correctly infer a private parameter from observable data. It aims to ensure that even with observed data, the adversary?s ability to infer private information is significantly limited."
Information Content (IC), "[]", "['Data Quality']", "Unique information content per data packet.", "Quantifies the amount of unique or novel information in each data packet. It is based on the probability of an event captured by the packet, where less probable (more unique) events hold higher information content. Data packets with low redundancy, often representing unexpected or rare events, have a higher IC value, making them more valuable for real-time and relevant data applications."
Information Diffusion,"['Data Distribution', 'Information Distribution']", "['Learning and Growth']", "Speed and scope of information sharing across entities.", "Addresses the flow of information across various entities, such as individuals, organisations, or systems, and how effectively, quickly, or broadly it can be shared or accessed. Key components include: Scarcity: The availability or rarity of the information; scarce information is less likely to spread widely. Sharing: The extent to which information is openly shared or kept private. Open data and social sharing platforms promote wider diffusion (i.e. linked to data governance). Infrastructure: The technological systems and networks (e.g., databases, cloud systems) that enable information transfer. Channels: The media or platforms (e.g., email, social media, and enterprise systems) through which information flows, is presented, or advertised."
Information Frequency, "[]", "['Data Quality']", "Rate at which information is updated, accessed, or used.","Measures how often information is updated, accessed, or used. It is more about the rhythm or rate of interaction with the information."
Integrity,"['Reliability', 'Data Prevention', 'Data Source', 'Corroboration']", "['Data Quality']", "Accuracy, consistency, and reliability of data across its lifecycle.","Data integrity ensures the accuracy, consistency, and reliability of data throughout its lifecycle, from creation to storage and retrieval. Maintaining data integrity is crucial for ensuring the data is trustworthy and useful for decision-making, analysis, and operations."
Internal Rate of Return (IRR), "[]", "['Data Monetization']", "Rate that makes the net present value of cash flows zero.","Internal Rate of Return (IRR) is a financial metric used to evaluate the profitability of an investment. It represents the annualised rate of return that makes the net present value (NPV) of all cash flows (both incoming and outgoing) equal to zero. In other words, IRR is the discount rate at which the present value of an investment?s costs equals the present value of its benefits."
Interoperability,"['Compatible', 'Integration Capabilities']", "['Operational Efficiency']", "Ability of systems to seamlessly share and utilize data across platforms.", "Ability of different systems, applications, or dataset to seamlessly be integrated and/or work together. It ensures that data can be shared, understood, and utilised across various platforms and environments. Key aspects to estimate this as a metric or KPI include Concordance of Data Quality and Governance factors (e.g. same Granularity or similar to a settled standard or Schema)."
IT Equipment Energy Efficiency, "['ITEE']", "['Operational Efficiency']", Measures useful work performed by IT equipment per unit of energy consumed.,The ITEE metric focuses on the useful work related to IT equipment (as opposed to DCPE which focuses on the data centre as a whole).
IT Equipment Utilisation, "['ITEU']", "['Operational Efficiency']", Ratio of actual to rated energy consumption of IT equipment.,"The ITEU metric focuses on the ratio of actual energy consumption to the rated energy consumption of IT equipment, providing insight into the energy utilisation efficiency of the IT infrastructure."
IT Power Usage Effectiveness, "['ITUE']", "['Operational Efficiency']", PUE metric applied to IT equipment rather than the entire data centre.,The ITUE is a PUE-type metric for the IT equipment rather than for the data centre.
Latency,"['Uplink/Downlink Communication Latency', 'Interserver Communication Latency', 'Database Access Latency', 'Transaction Finality Time', 'Link/Downlink Communication Latency (UDCL)']", "['Operational Efficiency']", Time between a request and its response.,"Measures the delay between the time a request is made and the time a response is received, typically measured in milliseconds (ms). It can also refer to the time it takes for a system to respond to a service (e.g., transaction finality time)."
Learnability, "[]", "['Customer Needs and Satisfaction']", Ease with which new users learn to use platform features like data search and visualization.,"Assesses how easily users can learn to use the platform and perform tasks such as data search and visualisation. Learnability is evaluated based on user testing results, focusing on how simple and understandable the data and interface are for new users."
Leave-One-Out (LOO), "[]", "['Data Valuation Techniques']", Change in performance when a specific data source is removed.,A method for estimating the data value by measuring the difference in performance when a specific data source is different. The LOO value is calculated by comparing the prediction accuracy of a model trained with and without the specific data source (see equation).
Licensing,"['License Compliance', 'Free License', 'Licensing Restrictions']", "['Data Governance and Compliance']", Share of datasets published under specified licensing terms.,"Measures the proportion of datasets published under specific licensing terms, such as open licenses."
Lifecycle, "['Shelf life of data']", "['Innovation and Growth']", Expected active lifespan of a data source.,"The lifecycle refers to the length of time a source of information is expected to remain active. Information is perishable, and after the lifecycle ends, the node (source of information) is assumed to no longer generate value."
Lineage,,Data Governance and Compliance,Complete record of a dataset?s origin and all transformations.,"Refers to the entire history of the data, including its origin as well as all the transformations, movements, and processes it has undergone throughout its lifecycle."
Location YardStick Score,,Data Valuation Techniques,Contribution of location data to specific analytical tasks.,"Measures the contextual value of location data based on its contribution to specific analytical tasks, such as trajectory estimation."
Loss and Missed Opportunity Costs,Data root cause remediation,Data Valuation Techniques,Revenue and profit lost due to poor data quality.,"Corresponds to revenues and profits lost due to poor data quality. For instance, inaccurate customer email addresses can result in lower revenues as acquired customers cannot be reached for advertising campaigns."
Loss of  Information Value (LIV),Revenue Loss,Data Valuation Techniques,Total financial impact of losing or compromising information.,"A metric quantifying the financial impact of losing or compromising information, encompassing both the cost of reacquiring or replacing the information and the cumulative income lost over time due to its unavailability."
Maintainability,,Operational Efficiency,"Ease of managing, updating, and enhancing data systems.","Maintainability evaluates how easily data systems, databases, and datasets can be managed, updated, cleaned, and enhanced over time. It involves modularity, scalability, ease of updates, error management, documentation, automation, and adaptability."
Maintenance Frequency,,Operational Efficiency ,Frequency with which a dataset requires maintenance.,"The frequency with which a dataset requires maintenance, directly impacting operational costs."
Market Adjustment Factor,"['Discount Price', 'Full Price']",Data Monetization,Market?condition adjustment factor applied to base data price.,"A coefficient that adjusts the base market price depending on external market conditions. Reflects factors like market demand, competition, and dynamics influencing the final sale price."
Market Value of Information (MVI),Average financial contribution per record,Data Monetization,"Projected income from selling, leasing, or sharing information.","This metric calculates the potential income from selling, leasing, or sharing information. It incorporates time, price, exclusive price, and discount rate."
Mean Time to Data Loss,MTTDL,Data Quality,Expected time until data loss under current redundancy scheme.,A traditional metric estimating how long a system will operate before experiencing data loss. Useful for comparing redundancy schemes and estimating system reliability.
Metadata,"['Contextual Information', 'Documentation features', 'Profiling']",Data Quality,Detailed specification of required information format and medium.,"Describes the required information in detail, including preferences for format and medium."
Moderation,,Data Quality,Proportion of data points within the extremes of a normal distribution.,The Moderation metric tells you how many of your data points lie within the extreme ends of what you?d expect if the data were perfectly Gaussian (i.e. normally distributed).
Mutual Information,,Data Quality,Mutual information shared between two variables.,"Measures the mutual dependence between two variables, quantifying the amount of information shared. Applications include feature selection, data compression, and pricing."
NDG,,Data Governance and Compliance,Number of decisions linked to two simple goals in the hierarchy.,Metric related to Traceability. Total number of simple decisions that trace up two simple goals in the goal hierarchy.
NDGI,,Data Governance and Compliance,Number of decisions linked both up to goals and down to requirements.,Metric related to Traceability. Total number of simple decisions that trace up to simple goals and trace down to the information requirements.
NDI,,Data Governance and Compliance,Number of decisions tracing down to information requirements.,Metric related to Traceability. Total number of simple decisions that trace down to the information requirements.
Net Present Value,,Data Monetization,Present?value?based valuation of future cash flows.,Determines the future cash flows of an asset to calculate its value.
Network Metrics,,Operational Efficiency,"Operational-efficiency metrics for data-centre networks (e.g., stretch, size, utilisation).","These represent metrics that are specific to data centres operational efficiency and include: Diameter Stretch, Path Stretch, Maximum Relative Size, and Network Utilisation."
Network Traffic Overhead,,Operational Efficiency,Bandwidth and resource consumption per data transaction.,"Measures the bandwidth and resource consumption associated with data transactions, influencing costs and scalability."
NGD,,Data Governance and Compliance,Number of goals tracing down to decisions.,Metric related to Traceability. Total number of simple goals that trace down to simple decisions.
NGI,,Data Governance and Compliance ,Number of high-level goals tracing down to information requirements.,Metric related to Traceability. Total number of goals at higher levels that trace down to information requirements at lower levels.
NID,,Data Governance and Compliance ,Number of information requirements tracing up to simple goals.,Metric related to Traceability.  Total number of information requirements that trace up to simple goals in goal hierarchies.
NIG,,Data Governance and Compliance,Number of information requirements tracing up to goals at any level.,Metric related to Traceability.  Total number of information requirements at lower levels that trace up to simple/complex goals.
Node Value,NV,Data Valuation Techniques,Value contribution of a decision node weighted by data quality.,"A decision node in DBV marks a point in an organisation?s workflow where a particular piece of information informs a choice; its value is the information?s contribution to that decision, weighted by data quality."
Number of Sensitive Field,,Data Governance and Compliance,Count of governed data columns in a data store.,Measures the number of data columns governed by policies in a data store.
Objectivity,,Data Quality,Degree to which data and analysis are unbiased and evidence-based.,"Objectivity refers to the degree to which data and its analysis are free from bias, personal opinions, or subjective interpretations. Ensures conclusions based on data are valid, reliable, and replicable. Key aspects include unbiased data collection, consistent application of methods, and evidence-based conclusions."
Open Data Barometer,ODB,Data Valuation Techniques,"Global index measuring government open-data readiness, implementation, and impact.","The Open Data Barometer (ODB) is a global index created by the World Wide Web Foundation to measure how governments are using open data to promote transparency, innovation, and social impact. It evaluates three main areas: readiness, which looks at how prepared governments and societies are to support open data; implementation, which assesses the availability and quality of published data; and impact, which examines how open data benefits politics, the economy, and society. By combining these factors, the ODB ranks countries and highlights opportunities for improvement."
Openness,Sharing,Data Governance and Compliance,Degree to which datasets are openly licensed and accessible.,"Openness measures the degree to which datasets provide a confirmed open license and format. The metric evaluates factors like open licensing, format compatibility, metadata quality, and ease of access."
Operational Cost (OPEX),"['Maintenance Cost', 'System Cost', 'Storage Cost', 'Contractual Costs', 'Labor Costs', 'Utility Costs', 'Transaction Fees', 'Publishing Cost', 'Service Cost', 'Application Cost', 'Cost']",Data Monetization,"Recurring costs of data-related operations (storage, maintenance, security, labor, utilities).","Operational Expenditure (OPEX) includes the recurring costs necessary to keep an organisation running. These cover data-related activities such as storing, organizing, backing up, securing, and updating data. Costs vary depending on dataset size, tools, and organisational needs. In data centres, OPEX also includes hardware maintenance and contractual costs. Contractual costs are fixed payments to third-party providers for services like data processing, analysis, or infrastructure. These may involve cost-plus terms based on data volume or service time and can represent major expenses, especially when contractors work on-site. Managing these is key to data monetisation profitability. Data maintenance costs include: Storage, Management, Backup/Recovery, Security, Cleansing/Updating, Compliance, Optimisation. Labour costs can include hardware installation, staff for data handling and system maintenance, consulting services, and training expenses. Utility costs cover electricity, cooling, equipment maintenance, and other overhead such as safety gear, transport, and dedicated server or office spaces used solely for Big Data operations."
Other Utilities,,Operational Efficiency,Efficiency of thermal and air management in data-centre operations.,"Other Utilities (e.g. Thermal and Air management metrics) fall under Operational Efficiency alongside HVAC performance measures. Key indicators include the Recirculation Index (RI), which quantifies how much hot exhaust air is recirculated rather than expelled; the Capture Index (CI), which assesses how efficiently the cooling system directs exhaust back into the intake without mixing with supply air; and the Data Centre Cooling System Efficiency (DCCSE), the ratio of cooling infrastructure power to total IT load. See the cited references for a complete list of metrics."
Oversight,Audit,Governance and Compliance,Capability to monitor and audit compliance with governance policies.,"Assesses an organisation?s capability to monitor and validate compliance with governance standards, processes, and policies. It includes mechanisms for periodic audits to ensure data integrity and proper governance implementation."
Ownership,,Data Governance and Compliance,Ownership clarity and licensing restrictions of a dataset.,"This metric evaluates the outright ownership of the data set, including any licensing restrictions and service agreements. Ownership impacts the value of the data based on who controls it and under what terms it can be used or shared. Assessed based on licensing terms and ownership rights detailed in service agreements."
Payment-Accuracy Tradeoff,,Data Valuation Techniques,Trade?off between payment for private data and accuracy of insights.,"The Payment?Accuracy Tradeoff as a KPI measures the balance between the cost of compensating individuals for sharing their private data and the accuracy of the insights gained from that data. It reflects how much payment is required to achieve a desired level of accuracy in data analysis, considering the privacy constraints."
Performance per Watt,PpW,Operational Efficiency,Actual energy efficiency of every device based on usage.,Measures the actual energy efficiency of every device in the data centre and how it is used.
Plausibility,"['Credibility', 'Believability', 'Match Between System and the Real World']",Data Quality,Degree to which data is credible and aligns with expected patterns.,"Plausibility in the context of data refers to the degree to which the data is reasonable, credible, and aligns with expectations based on known relationships, patterns, or rules. It assesses whether the data makes sense within the context it?s being used, ensuring that it reflects real-world situations or follows logical assumptions."
Policy,,Data Governance and Compliance,High-level principles guiding organizational decision-making.,Policies are high-level principles or rules that an organisation establishes to guide decision-making and behaviour.
Power Density Efficiency,PDE,,Improvement in energy efficiency from rack-level physical changes.,"A variation of PUE that can provide insight into the improvements to both the IT equipment and the supporting cooling system. The proposed metric enables evaluation of the impact of physical changes inside the racks on energy efficiency, which is not possible using the common metrics above."
Power Usage Effectiveness (PUE),"['Carbon Usage Effectiveness (CUE)', 'Total Energy Usage Effectiveness (TUE)', 'Data Centre Efficiency (DC)', 'Data Centre Infrastructure Efficiency (DCiE)', 'SPUE', 'pPUE', 'PUE1?4', 'SI-POM', 'H-POM']",Operational Efficiency,Ratio of total facility power to IT equipment power (PUE) and related carbon and conversion metrics.,"PUE is probably the most well-known data centre efficiency metric and is defined in ISO/IEC 30134-2 / EN 50. SI-POM and PUE are essentially equivalent metrics used to evaluate the energy efficiency of a data centre?s infrastructure by comparing total facility power consumption to IT equipment power consumption. The IT H-POM metric, on the other hand, focuses on the efficiency of power conversion within IT equipment, aiming to minimise the overhead power relative to the actual compute power used. Directly connected to PUE is CUE which measures the carbon emissions associated with the energy consumption of a data centre. It quantifies how much carbon dioxide (CO2) is emitted per unit of energy used by IT equipment"
Precision,,Data Quality,Level of detail with which data is captured and represented.,"Precision refers to the level of detail with which data is captured, measured, and represented. It describes how specific the data values are and impacts the reliability of results and analyses. Further concepts are provided in the main document."
Privacy Budget,,Data Governance and Compliance,Privacy budget (�) controlling trade-off between privacy and accuracy.,"The privacy budget quantifies the amount of privacy loss tolerated in a system. It is denoted by ? (epsilon), which controls the trade-off between privacy and accuracy in differential privacy algorithms. A smaller ? provides stronger privacy guarantees at the expense of data utility. By treating ? as a budgeted resource, organisations can balance data utility against individual privacy guarantees in a transparent, mathematically grounded way."
Privacy Level,"['Privacy Sensitivity', 'Propensity Score']","['Data Governance and Compliance', 'KPIs and Metrics for Data Monetisation']",Privacy risk measured by how closely synthetic data resembles real data.,"Measures the privacy risk or leakage in data reporting. It helps assess or control the trade-off between privacy, data utility, and the cost of compensating individuals for privacy loss. A lower propensity score indicates synthetic data closely resembles real data without revealing sensitive information."
Process Failure Costs,,Data Valuation Techniques,Frequency of process failures caused by poor data quality.,"Poor quality data causes processes to fail. For example, inaccurate mailing addresses cause correspondence to be misdelivered, algorithms to fail, or ML/AI component to have low accuracy or misled results."
Processing Value Ratio,,Data Valuation Techniques,Value added by processing data into information.,Determines the value added through processing data into information. It is calculated differently based on the type of value transfer.
Protection Expense,,"['Data Governance and Compliance', 'Data Valuation Techniques']",Cost to implement and maintain data protection measures.,"Cost to apply protection measures (e.g., encryption, access control) to specific data stores. Includes costs of implementing security controls, maintenance, and resource allocation."
Proximity,,Data Quality,Proximity relevance of data based on event or sensor location.,"Proximity is important in applications like fog computing and habitat monitoring, where the location of events or sensors affects data valuation."
Quality Factor (QF),,Data Quality,Link between information quality and its business innovation potential.,"Links overall quality of information, including accuracy and frequency, to its potential value for business innovation and growth."
Quality of Service (QoS),"['Data Connected to Service Levels', 'Service Characteristic Index', 'Service Level Agreement']",Customer Needs and Satisfaction,Performance level and reliability as defined by SLAs.,"Refers to the performance level and reliability expected by clients when accessing datasets. Key points include latency constraints (ensuring acceptable latency for reconstructing datasets) and adherence to Service Level Agreements (SLAs), which outline expected performance metrics such as response times and availability."
Quantity of Private Projects,,Innovation and Growth,Number of private vs. public projects/services/datasets available online.,Represents the quantity of private projects/services/datasets available online. 
Quantity of Public Projects,,Innovation and Growth,Proportion of data within valid lower and upper bounds.,Represents the quantity of public projects/services/datasets available online. 
Range,,Data Quality,Cost or effort to restore lost or corrupted data.,"Quantifies the proportion of data values that fall within predefined lower and upper bounds, reflecting the validity of data within expected limits."
Reconstruction Cost,Packet Recovery Score,Data Valuation Techniques,Effectiveness of packet recovery during transmission.,"Reconstruction Cost refers to the financial or computational effort needed to restore lost or corrupted data, typically from backups or redundant sources. It reflects the cost-effectiveness of recovery in systems focused on data integrity and availability. Packet Recovery Score, often used in networking contexts, measures how effectively lost or corrupted packets are recovered during transmission. It indicates the resilience of systems like IoT or wireless sensor networks against packet loss."
Regulatory Compliance,Compliance Cost,Data Governance and Compliance,Degree of legal and regulatory compliance of data.,"Measures whether the data complies with relevant legal and regulatory standards. Estimated financial cost to business of not keeping data for compliance/regulation purposes. It can be defined as a quantitative or qualitative measure that evaluates the extent to which data adheres to the predefined rules, standards, or requirements. Compliance metrics focus on ensuring data consistency, reliability, and alignment with organisational or regulatory frameworks."
Relevance,"['Decision  Support Capabilities', 'Relevance Factor', 'Priority Score', 'Importance', 'Existence']",Data Valuation Techniques,Usefulness and adaptability of data for business processes.,"Refers to the usefulness of data for business processes, ensuring data is meaningful, reusable, and adaptable to changing demands. Includes relevance factor, priority score, and existence checks for completeness."
Renewable Energy Factor (REF),"['On-site Energy Fraction (OEF)', 'On-site Energy Matching (OEM)']",Operational Efficiency,"Share of energy consumption met by renewables, on-site and grid.",Measures the proportion of a data centre?s total energy consumption that is sourced from renewable energy. OEF evaluates the share of a data centre?s energy needs met through on-site renewable energy generation. OEM assesses how well the on-site renewable energy generation aligns with the data centre?s energy consumption patterns.
Reputation,Popularity,Market Penetration,"Perceived quality, governance, transparency, and ethical use of data.","Refers to the perceived quality, reliability, and trustworthiness of data, impacting decision-making and governance. Key aspects include quality, governance, transparency, and ethical use."
Response Time,,Operational Efficiency,Total latency from request initiation to system response.,"Total duration from the initiation of a request to the system?s response. It encompasses multiple factors such as processing time, communication latency, database access latency, transaction processing speed, and network-induced delays."
Responsiveness,"['Time Metrics', 'Speed']",Operational Efficiency,Responsiveness of a system to inputs beyond raw processing speed.,"Responsivenes is more than response time. Relates to the responsiveness of a system or service, measuring data processing speed and timely updates. Speed refers to the raw processing capability of a system?such as how quickly a database query is executed?responsiveness emphasises the system?s ability to react promptly to user inputs or requests."
Return on Investment (ROI),"['Profit', 'Estimated Benefit to Business from Using Data']",Data Monetization,Profitability or efficiency ratio of an investment.,"Measures the profitability or efficiency of an investment, comparingnet gains to costs."
Revenue,Economic Benefits,Data Monetization,Total income from normal business operations.,Refers to total income from normal business operations. 
Risk Cost,Regulatory Risk,"['Data Governance and Compliance', 'Data Valuation Techniques']","Financial impact of a data breach, including fines and losses.","Measures the financial impact of a potential data breach, including fines, legal fees, reputation damage, and operational disruption. Risk Cost is defined as the total monetary impact incurred by an organisation if a data breach, compromise, or loss occurs. It is computed by quantifying various financial and reputational damages and multiplying these by the number of records at risk."
Risk Score,Risk Management Index,Data Governance and Compliance,Overall risk score of a data store based on multiple risk factors.,"Refers to the systematic process of identifying, assessing, and mitigating risks associated with data assets. Risk score is calculated as a weighted sum of multiple risk factors, including sensitivity level and protection percentage, to reflect the overall risk level of a data store."
Rival Access Loss,,Data Valuation Techniques,Estimated cost if competitors access the data.,Estimates the financial cost to a business if competitors gain access to its data.
Runtime,Processing Time,Operational Efficiency,Runtime overhead relative to a performance baseline.,Refers to the time taken to execute tasks or processes in a data system. Runtime overhead evaluates performance improvements or degradations relative to a baseline.
Satisfaction,"['Feedback', 'User?s Satisfaction', 'User Attitude', 'Behaviour', 'Business User Satisfaction', 'Degree of Satisfaction', 'Business User Satisfaction']",Customer Needs and Satisfaction,"User satisfaction with interface, layout, and data presentation.","Measures user satisfaction with the platform interface, layout, and data presentation. Combines objective metrics like accuracy and relevance with subjective feedback to gauge satisfaction."
Scalability,"['System Concurrence Processing Capabilities', 'Elasticity']",Operational Efficiency,System?s ability to scale resources dynamically with demand.,"Evaluates the ability of a system to handle increased data volumes. Elasticity allows dynamic allocation of resources to meet demand, ensuring performance and cost-efficiency."
Scarcity,,Data Monetization,Availability or rarity of information for monetization.,"Measures the availability or rarity of information.  Scarce information may have limited distribution, impacting its monetisation potential."
Schema,,Data Governance and Compliance,Presence of desired data attributes like clarity and robustness.,"Defines the presence or absence of desired attributes, including clarity, comprehensiveness, flexibility, robustness, and precision of domains."
Security Composite Efficiency Indicator,,Data Governance and Compliance,"Composite cybersecurity efficiency score (readiness, resilience, penetration).","Combines multiple efficiency indicators to assess overall system performance in cybersecurity. Metrics include equipping coefficient, technical readiness, and penetration testing resilience."
Security Level Index,"['Security', 'Access Security']",Data Governance and Compliance,"Strength of data protection measures (confidentiality, integrity, availability).","Measures the protective measures implemented to safeguard data from unauthorised access and breaches. Key aspects include confidentiality, integrity, availability, and compliance with regulations."
Server Compute Efficiency (ScE),,Operational Efficiency,Compute efficiency of data-centre resources per energy consumed.,Designed to assess the efficiency of compute resources within a data centre. The metric aims to evaluate how effectively servers perform computational tasks relative to the energy they consume.
Service Agreement,,Governance and Compliance,Clarity and enforceability of data?usage contracts and licenses.,"A formal contract defining data ownership, licensing restrictions, compliance standards, and usage rights, evaluated for clarity, regulatory alignment, and flexibility to support data utility and accessibility."
Shapley Fairness,Fairness Metrics,Data Quality,Fair allocation of resources or data access among users or groups.,"Related to allocating resources proportionally in systems like VMs or ensuring data fairness, e.g., reducing biases."
Shapley Value,,Data Valuation Techniques,Marginal contribution of each data source in a coalition.,"A baseline metric to measure the importance of each data provider, dataset, or feature in a coalition of data sources. Reflects the marginal contribution of each data source to the overall system, providing a fair value and reward mechanism."
Social Welfare,Social Benefits,"['Innovation and Growth', 'Customer Needs and Satisfaction']",Net utility gain minus privacy cost in a data marketplace.,Represents the difference between the aggregate utility of all model requesters and the total privacy cost of all data owners in a data marketplace.
Space Cost,Disk Occupation,Data Valuation Techniques,Storage space required per unit of data.,"Amount of storage space required to store data, which can vary depending on the file format and storage mechanisms used. The cost can be linked later to the cost associated with maintaining such information."
"Space, Wattage, and Performance. (SWaP)",,Operational Efficiency,Overall facility resource usage including support infrastructure.,Evaluates the efficiency of a system by considering its performance output relative to the space and power it consumes.
Statistical Parity,,Innovation and Growth,Fairness in ML via equal positive?class probability for sensitive groups.,A fairness metric in ML ensuring equal probability of inclusion in the positive predicted class for sensitive groups. Synthetic data can be adjusted to meet statistical parity requirements.
Stochastic Divergence,"['Identity-based Exact Match', 'Jensen-Shannon Divergence', 'Wasserstein Distance']",Data Quality,Measure of distribution similarity or divergence between datasets.,"Stochastic Divergence is a measure of the difference or similarity between statistical distributions, focusing on their probabilistic or statistical characteristics. It encompasses metrics that assess how closely two distributions align or diverge."
Storage Cost,Cost of data Storage,Data Valuation Techniques,Infrastructure and management costs of data storage systems.,"Storage costs cover the infrastructure, management, and upkeep of data systems, including hardware, cloud services, energy use, and compliance. Poor management of redundant or unused data can greatly inflate these costs, especially in data lakes and enterprise systems."
Stranded Power Capacity Per Rack (SPCR),,Operational Efficiency,Unused power allocated within server racks.,Allocated but unused power within individual server racks.
Structure,Data Structure,Data Quality,Organization and format of stored data for efficient access.,"Defines the organisation and storage format of data, enabling efficient access, modification, and management."
Success Rate / Ratio / Count,,Operational Efficiency,Operational success rate: completed tasks versus attempts.,"Proportion of operations, tasks, or processes successfully completed relative to total attempts. Evaluates system reliability and effectiveness."
Support,,Operational Efficiency,Support for database rule mining: proportion of records matching rules.,"Proportion of records in the dataset that satisfy both the antecedent and consequent of a rule, highlighting common patterns."
Synchronisation,,Data Quality,Degree of synchronization between multiple data sources.,"Refers to the process of ensuring that two or more data sources (like databases, devices, or systems) have the same, up-to-date information Can be explicitly defined in quantitative terms based on its role in assessing the alignment and coherence of systems or datasets."
Syntactic Similarity,"['Levenshtein Distance', 'Edit Distance', 'Cosine Similarity', 'Q-gram Distance', 'Semantic Similarity']",Data Quality,String similarity metrics for data value comparison.,"Metrics to assess how similar data values are in terms of their syntax, e.g., based on character similarities. Levenshtein Distance is used as a primary metric to calculate this similarity. Additional metrics include Edit Distance, Cosine Similarity, Q-gram Distance, and Jaccard Coefficient."
System Capacity,"['Memory', 'CPU', 'Bandwidth', 'Storage Capacity']",Technology and Infrastructure,Maximum load capacity before performance degradation.,"Refers to the maximum amount of work, load, or use that a system can handle without significant degradation in performance. Includes CPU, memory, storage, and bandwidth capacity. This information can be used also to generate ratios for System Utilisation metric."
System Robustness,System Stability,Operational Efficiency,System stability and resilience under disturbances or attacks.,"Measures the ability of a system, model, or process to remain stable and perform well despite disturbances, faults, or unexpected inputs. Key aspects include system stability, resilience to adversarial attacks, error handling, and generalisation."
System Utilisation,"['CPU Utilisation', 'Memory Utilisation', 'Disk Utilisation', 'Deployed Hardware Utilisation (DH-UR)']",Operational Efficiency,"Utilization rate of CPU, memory, storage, and network resources.","Measures how efficiently and effectively a system?s resources?such as CPU, memory, storage, and network are being used during a given period."
The Green Index (TGI),,Operational Efficiency,Flexible green benchmarking metric based on performance-per-watt.,"Metric offers flexible green benchmarking. While defined using performance-per-watt, it can incorporate any energy-efficiency metric. TGI focuses solely on IT equipment power but can be extended to include cooling infrastructure."
The Value of Information for Business (VIB),,Data Valuation Techniques,"Actionable information quality (accuracy, relevance, timeliness).","Assesses how useful information is for business processes, focusing on accuracy, completeness, relevance, and delay in receiving the information."
Throughput,Transaction Processing Speed,Operational Efficiency,Throughput: number of requests processed per time unit.,"Measures the number of requests a system processes over a specific period (e.g., requests per second, minute, or hour)."
Timeliness,Data Freshness,Data Quality,Data availability and timeliness for user needs.,"Measures the availability of data when needed, ensuring data is up-to-date and accessible within an appropriate timeframe. Includes frequency of updates, and speed of availability."
Total Equipment Utilisation (TEU),,Operational Efficiency,Overall resource usage of the entire facility.,"Could be considered within System Utilisation Metric but given its broad use, has been set apart. It encompasses the overall facility?s resource usage. It includes cooling, power distribution, and other supporting infrastructure components."
Traceability,"['Addressability', 'NGD', 'NDI', 'NDG', 'NID', 'NDGI', 'Verifiability', 'Provenence Documentation', 'Audit Trail Coverage', 'Compliance Rate']",Data Governance and Compliance,"Ability to trace data lineage, provenance, and audit trails.","Refers to the ability to track and verify data throughout its lifecycle. It involves understanding data lineage?how data flows from its source through various transformations to its final use?and provenance, which captures the origin and history of the data, including any changes made. Maintaining audit trails ensures that all modifications are recorded, including who made them, when, and what was changed, supporting accountability and transparency. Traceability also plays a key role in meeting regulatory and governance standards by demonstrating data integrity and responsible data handling. Additionally, addressability defines how the origin of data can be identified and, if needed, contacted or referenced.  Other metrics useful to estimate traceability includes Data Lineage Completeness measures the percentage of data elements for which lineage information is available. Provenance Documentation assesses the percentage of data elements with documented provenance. Audit Trail Coverage evaluates the percentage of data changes that are recorded in an audit trails. Compliance Rate measures the percentage of data processes that comply with established governance and regulatory standards."
Traffic Energy,Management and Monitoring Traffic Energy (MMTE),Technology and Infrastructure,"Energy footprint of data collection, transmission, and processing.","Accounts for the power required by sensors to generate data, the energy used for data transmission across multiple network hops, and the computational resources needed to store and process incoming information. Additionally, some specific component of data management can be measured."
Traffic Ratio,"['Management and Monitoring Traffic Ratio (MMTR)', 'Internal Traffic', 'External Traffic']",Technology and Infrastructure,Proportion of specific traffic type in total network data.,"Represents the proportion of a specific type of network traffic relative to the total traffic in a network or data centre. It is used to analyse the composition of traffic, distinguish between different categories (e.g., internal vs. external, management vs. application-specific)."
Transparency,Objective Measurement,"['Innovation and Growth', 'Market Penetration', 'Data Governance and Compliance']",Clarity and interpretability of data and processes.,"Refers to the ease of identifying, understanding, and interpreting data and processes. Includes clarity in presenting results and consistency in analysis."
Trustworthiness,"['Assurance', 'Trust Score']",Innovation and Growth,Trustworthiness of data based on reputation and credibility.,"Refers to the believability or trustworthiness of data, often based on its integrity, reliability, and consistency. Trust Score (TS) is derived from reputation and credibility without involving a centralised authority."
Typicality,,Data Quality,Degree of conformity of a data point to expected patterns.,Measures how well a data point aligns with expected or ?typical? patterns within a dataset. It helps identify outliers or unusual events using statistical or machine learning methods. Use statistical distributions and statistical tests to check it.
Understandability,,Data Quality ,Clarity of dataset components and metadata for user interpretation.,"Reflects how clearly a dataset and its components?such as field names, units, and metadata?can be interpreted and used by the intended audience. It involves ensuring that field names are clear and unambiguous, units are explicitly defined and consistently applied, and metadata offers sufficient context to describe data structure and relationships. It also includes identifying barriers to interpretation, such as inconsistent formatting or missing explanations."
Uniformity,,Operational Efficiency,Uniformity of data representation across systems or datasets.,"Uniformity as a metric measures the consistency of data representation across datasets, systems, or within a single dataset. "
Uniqueness,"['Percentage of Duplicate Data', 'Concentration', 'Redundancy']",Data Quality ,Uniqueness of records to avoid duplication.,"Ensures that each record in a dataset is distinct, avoiding duplicates. This also can be linked to data systems, in which the reference of Redundancy is linked to the number of repeated datasets that support System Robustness."
UPS Metrics,"['USF', 'UCF', 'UPFC', 'UPF', 'UPEE']",Operational Efficiency,UPS energy-conversion efficiency and related performance metrics.,"Metrics related to the performance of Uninterruptible Power Supply. These metrics are general energy efficiency or performance metrics related to that unit, such as Energy Efficiency (UPEE), which measure of how UPS converts input power into usable output power while minimizing energy losses."
Usability,"Usage, ease of use, friendliness",Data Quality ,"Ease of accessing, understanding, and using quality data.","The ease and efficiency with which quality data (Data-Value, Accuracy, Integrity, Completeness) can be effectively accessed (Communnication, Accessibility, Timeliness), understood (Clarity), and correctly be utilised (Easy-to-use or Utilisation & Performance, Relevance or Utility) by users to accomplish specific tasks."
User Frequency,"['User Count', 'Concurrent Users']",Customer Needs and Satisfaction,Concurrent user count and associated incremental cost.,"Tracks the number of concurrent users accessing data, with an associated cost for each additional user."
Utility,"['Relevance', 'Application Characteristic Index', Retention']",Data Valuation Techniques,Functional contribution of data to achieving intended goals.,"Utility, in the context of data valuation, refers to how effectively a resource?such as a dataset or system?supports the achievement of intended goals. Unlike usability, which focuses on ease of interaction, utility measures functional contribution to outcomes. While related to relevance, utility is broader; relevance is domain-specific and tied to particular use cases, whereas utility reflects overall potential impact."
Validity,,Data Quality,Adherence of data to defined business rules and criteria.,"Refers to the compliance of the information with the business rules that describe it (e.g. the age of a person must be Integer). It seems similar to Veracity but it covers the aspect of accuracy and precision of the data concerning the intended use. Validity is defined as the adherence of data values to specified criteria, ensuring that each data entry is logically sound and meets predefined conditions. This is typically done by applying specific rules to the dataset. For example, a rule might state, ?Asset Cost in any Asset record must be greater than zero? which would flag any entries with a non-positive asset cost as invalid."
Value Added,"['Value-added', 'Diminishing Value']",Data Valuation Techniques,Temporal decay of data?s utility over time.,"Value-Added and Diminishing Value metrics reflect the relevance and economic utility of data over time. While Value-Added emphasises the contribution of data to decision-making or operational efficiency, Diminishing Value highlights the temporal decline in data?s utility, particularly critical in sectors like IoT and real-time monitoring systems, where up-to-date information is essential for maintaining competitive advantage."
Value of Privacy,Privacy cost,Data Monetization,Minimum payment required for specific privacy level.,Minimum payment required for sharing data at a specific privacy level (?). Balances privacy loss with data utility.
Value Range,,Data Monetization,Potential value range achievable with required information.,"Describes the potential value achievable using required information. This value is assessed within a range, where moderation ensures values remain within reasonable or typical limits. Based on the DBV method, it incorporates an income approach for estimating benefits of intangible assets."
Variety,Multifacetedness,Data Quality ,Diversity of data types and formats.,Refers to the diversity of data types and formats. Variety emphasises the complexity and richness of data attributes for comprehensive analysis.
Velocity,Frequency Parameter,Data Quality,"Rate of data generation, ingestion, and processing.","Velocity describes the rate at which data is generated, ingested, and processed over time."
Views,,Market Penetration,Number of times a dataset?s page is viewed.,Represents the number of times a dataset?s page is accessed by users.
Visualisation,,Customer Needs and Satisfaction,Effectiveness of graphical data representations.,"Refers to the graphical representation of data using charts, graphs, maps, and diagrams. Effective visualisations improve clarity, accuracy, and relevance for quick insights and decision-making."
Volatility,,Data Quality ,Variability of data values over time.,"Volatility, as a data quality metric, refers to how frequently data changes or how long it remains valid before becoming outdated. It is closely linked to timeliness and currency, and may also indicate how often data deviates from expected values or business rules. In statistical and financial contexts, volatility is typically measured as the standard deviation or variance over time, reflecting the magnitude of fluctuations in a variable?s value."
Volume,"['Quantity', 'Entries']",Data Quality,Total volume of data available for analysis.,"Represents the total amount of data available for analysis, influencing reliability, pattern recognition, and machine learning. Adequate quantity must pair with high quality for actionable insights."
Wasserstein,Wasserstein Distribution,Data Quality,Degree of deviation between empirical and true distributions.,"Measures the difference between two probability distributions, quantifying how much the empirical distribution deviates from the true distribution."
Water Usage Effectiveness ,WUE,Operational Efficiency,Water usage per unit of IT energy consumption.,It measures the amount of water a data centre uses in relation to the energy consumed by its IT equipment.
Weighted Coverage Function,,Data Valuation Techniques,Price?adjustment weights for database query pricing.,"This function assigns weights to database instances, adjusting prices for queries based on the seller?s inputs. It reflects information disclosure."
Winning Rate,,Customer Needs and Satisfaction,Winning rate of data providers in query allocation.,"Winning rate is defined in the context of online ad allocation as a fairness metric. Measures the percentage of queries a data provider successfully serves, relative to the queries they are eligible to serve. Higher rates indicate valued offerings."
