name, alternative_names, BSC_subfamilies, definition, explanation
Access Cost, "[Bandwitdh Cost]", "['Data Valuation Techniques']", "The cost (not necessarily the price) incurred for accessing a dataset.", "The cost (not necessarily price) incurred for accessing a dataset based on historical access logs and the cloud provider's pricing (if existing). This metric directly refers to the cost incurred when accessing data, which is influenced by bandwidth, especially in geodistributed data systems where data needs to be transferred over long distances."
Accessibility, "[Search, Functionality, Navigation and Link]", "['Data Valuation Techniques', 'Data Governance and Compliance', 'Technology and Infrastructure']", "Accessibility in data and information systems refers to how easily users can access, understand, and utilize data—factoring in availability, usability, interoperability, inclusivity, security, licensing, and standardized formatting to maximize its value and usability.", "Accessibility, in the context of data and information systems, refers to the ease with which users can access and utilize data. It encompasses several key aspects:\nAvailability: Data should be readily available to authorized users when needed, without unnecessary delays or barriers.\nUsability: The data should be presented in a format that is easy to understand and navigate, allowing users to efficiently find and use the information they need.\nInteroperability: Data should be compatible with various systems and applications, enabling seamless integration and exchange of information across different platforms.\nInclusivity: Accessibility also considers the needs of diverse user groups, including those with disabilities. This means ensuring that data and information systems are designed to be usable by individuals with varying abilities, such as providing alternative formats or assistive technologies.\nSecurity and Permissions: While ensuring accessibility, it is also important to manage access controls and permissions to protect sensitive data from unauthorized access.\nOpen to use / licensing: There is no limit on users to use the data.\nFormat: Follow prestrucutred definitions for the data content strucutre, for example, does the dataset match Berners-Lee's 'linked data principles'?\nOverall, accessibility is crucial for maximizing the value of data, as it ensures that users can effectively leverage the information for decision-making, analysis, and other purposes. Measures how accessible the platform is for users in terms of ease of navigation, search, and data retrieval."
Accuracy, "[]", "['Data Quality']", "Accuracy is a multifaceted measure of how well data aligns with true or expected values, encompassing elements like precision, relevance, timeliness, completeness, and context to ensure reliability and validity in achieving research objectives.", "Accuracy in the context of scientific analysis is a comprehensive measure of how closely data or results align with the true, intended, or expected values. It encompasses multiple facets that ensure the reliability, applicability, and validity of the information within the specific scientific domain. The accuracy of data or results is not only about correctness but also about their overall suitability for achieving the desired objectives in a given context.\nThe complexity of the relationship between accuracy and the value gained depends on the type of information and the organization. Moreover, unless specified, the basic calculation of value obtained by meeting the required accuracy. As defined by [12] Accuracy is depends on Range, Consistency, Typicality and Moderation Metrics. Nevertheless, different key elements can also be used to explain accuracy:\nPrecision: Refers to the granularity or level of detail in the data. In science, precision ensures that measurements are not only correct but also detailed enough to provide meaningful insights. For example, reporting a temperature as 37.456°C instead of rounding to 37°C can be crucial in laboratory experiments.\nTimeliness: Information must reflect the most current state. In scientific research, outdated data can lead to incorrect conclusions, making the timeliness of updates or recalibrations essential.\nRelevance: Accuracy is context-dependent. Data must be both correct and directly applicable to the research question or hypothesis. For instance, accurate but irrelevant data adds no value to the scientific inquiry.\nCompleteness: Scientific accuracy requires that no critical data points are missing. Incomplete datasets can lead to skewed or biased results, undermining the validity of findings.\nTraceability: The provenance of data ensures it can be verified and validated. For science, traceability means being able to track data back to its source, such as instrumentation or observation records, to confirm its authenticity.\nTolerance and Range: Recognizes that minor inaccuracies or deviations may be acceptable within specified thresholds. Scientific accuracy involves defining these thresholds to minimize the impact on overall validity. Furthermore, in terms of range it specifies limits for acceptance too.\nContext Dependence:  Accuracy must meet the specific standards and expectations of the scientific discipline. For example, a minor deviation in physics experiments might be acceptable, whereas in clinical trials, even small inaccuracies can have serious consequences.\nConsistency: Data should be reliable and uniform across various trials or applications. Inconsistencies in repeated experiments can signal issues with data accuracy.\nTypicality: Measures whether the data reflects typical or expected conditions. Atypical data may require additional validation to ensure its accuracy.\nModeration: Ensures data is not skewed or biased toward extremes. Balanced data contributes to overall scientific accuracy by avoiding distortions."
Adaptability, "['Versatility', 'Flexibility']", "['Operational Efficiency']", "Relates to how well a system can adjust to changes in its environment, such as varying network conditions, changing user needs, or different operating contexts.", "Relates to how well a system can adjust to changes in its environment, such as varying network conditions, changing user needs, or different operating contexts. Adaptable systems make scalability more efficient because they can dynamically adjust their resources and behavior to accommodate growth. "
Age, "['Data Creation', 'Time Index', 'Age of Information']", "['Data Quality']", "Refers to how recent or old the data is.", "Refers to how recent or old the data is. The age of data can impact its relevance and accuracy, especially in fast-moving fields like technology or finance."
Availability, "['Retrievability']", "['Operational Efficiency']", "Data availability refers to the reliable, timely, and uninterrupted access to data by authorized users, supported by system robustness, fault tolerance, backups, and redundancy to ensure operational continuity and effective decision-making.", "Data availability refers to the degree to which data is accessible and retrievable when needed, by authorized users or systems. It ensures that the data can be accessed in a timely manner without interruption, and is crucial for maintaining smooth operations, decision-making, and service continuity. As a KPI it could have the following Key Aspects:\nAccessibility: Ensuring that the data is available to authorized users when they need it, whether for operational or analytical purposes. (i.e. binary per dataset) \nReliability (Measure throughout System Robustness): The data should be consistently available without frequent downtime or failures.\nFault tolerance (Measured through System Robustness): Systems storing the data should be resilient to hardware or software failures, ensuring minimal disruption in data access. \nBackup: Implementing regular backups and quick recovery solutions ensures that data is still available after incidents like data corruption or system failures.\nRedundancy: Data availability can be enhanced through redundant systems (e.g., having multiple copies of the data across servers), so if one system fails, another is still operational."
Backup, "['System Backup', 'Recovery Capabilities', 'Data Redundancy', 'Reduction']", "['Operational Efficiency']", "A system's ability to securely preserve data and ensure its availability for recovery during disruptions, emphasizing robust mechanisms and minimal data loss.", "A system's ability to securely preserve data and ensure its availability for recovery during disruptions, emphasizing robust mechanisms and minimal data loss. The opposit perspective of backup is Data Redundancy Reduction, that can be a metric when the objective is a scalable services (big IoT services) without considreable repetition, as mentioned in [105]."
Budget, "['Costs Planned', 'IT Plan', 'Plan']", "['Data Monetization]", "A budget is a financial plan that outlines expected income and expenditures over a specific period.", "A budget is a financial plan that outlines expected income and expenditures over a specific period. It represents an estimate of how much money an individual, business, government, or organization expects to earn (revenue) and how much it plans to spend (expenses) to achieve certain financial or operational goals."
Cache Size, "[]", "['Technology and Infrastructure']", "Cache size refers to the amount of data (e.g gigabytes) that a cache can store at any given time.", "Cache size refers to the amount of data (measured in units such as kilobytes, megabytes, gigabytes, etc.) that a cache can store at any given time. The cache is a high-speed data storage layer that stores a subset of data, typically temporary, to serve future requests more quickly than retrieving the data from its primary storage or origin location."
CAPEX, "['Capital Cost', 'Hardware Costs', 'Software/Application Costs', 'Service Cost', 'Infrastructure Unit Costs']", "[Data Monetization]", "CAPEX refers to one-time investments in long-term physical and technological assets—like hardware for data capture, storage, and processing—essential for building or upgrading data systems, with costs sometimes offset by asset depreciation or pre-installed software.", "CAPEX refers to the one-time, upfront investments made to acquire, upgrade, or maintain physical assets or infrastructure, independent on the number of quotes needed to cover this expense. These expenses are typically for assets that have a useful life beyond the current accounting period, such as:\n(1) Purchasing new machinery or equipment; (2) Constructing buildings or facilities; (3) Upgrading technology infrastructure (e.g., data centers, servers); (4) Acquiring servers or other long-term assets; (5) Investing in property or land.\nTo be more specific, hardware cost (past of CAPEX) for a data and information system encompasses the following components:\n(1) Data Capturing Hardware: Computers and devices used to collect and input data; (2) Acquisition Systems: Sensors, data acquisition (DAQ) systems, and similar tools for measuring and capturing values; (3) Information Collection Mediums: Devices like PDAs and tablets used for collecting data. (4) Data Storage Hardware: Systems for storing large volumes of data. (5) Data Processing and Analysis Hardware: Computers and servers for processing and analyzing data.\nIf existing hardware is repurposed, the cost should be based on its depreciation during use. If fully depreciated, the cost is effectively zero.\nFurthermore, A significant aspect of Data is the processing, storing, and viewing of it. This requires specialized software to accomplish unlike small data sets. This software can cost and needs to be incorporated into the total costs of the Data system. Although the cost of this software can be significant, other software such as Microsoft Office are often used for data analysis. These types of software are typically pre-installed on the computers bought by organizations and used by employees. Therefore, they normally run at zero cost for the node and can excluded from its costs."
Carbon Usage Effectiveness, "['CUE']", "['Innovation and Growth']", "A carbon metric used to measure the environmental impact by assessing the amount of CO2 emissions per unit of IT energy consumed. ", "A carbon metric used to measure the environmental impact by assessing the amount of CO2 emissions per unit of IT energy consumed."
Carbon Emission Factor, "['CEF']", "[Innovation and Growth']", "The amount of carbon dioxide (CO2) emissions produced per unit of energy consumed, typically expressed in kilograms of CO2 per kilowatt-hour (kgCO2/kWh).", "Coefficient used to calculate the amount of carbon dioxide (CO2) emissions produced per unit of energy consumed, typically expressed in kilograms of CO2 per kilowatt-hour (kgCO2/kWh). It varies depending on the energy source (e.g., coal, natural gas, renewable energy) and the region's energy mix."
Churn, "[]", "['Customer Needs and Satisfaction', 'Market Penetration']", "Churn refers to customer loss, and predicting it involves analyzing data such as demographics, usage patterns, transactions, interactions, sentiment, and external factors to help businesses retain customers proactively.", "The term churn refers to the loss of customers, and predicting churn helps businesses take proactive measures to retain customers before they leave. Data Used for Churn Prediction: \nCustomer Demographics: Age, gender, location, and other personal information that can give insights into churn behaviour.\nUsage Patterns: How often and in what way customers use the service (e.g., call frequency, data usage, purchase frequency).\nTransaction History: Purchase or payment records, subscription renewals, or cancellations.\nInteraction Data: Customer interactions with the company's customer service, support requests, or social media engagement.\nSentiment Analysis: Textual data from customer feedback, complaints, or reviews that can reveal dissatisfaction.\nExternal Data: Market trends, economic factors, or competitor actions that may influence customer churn."
Clarity, "['Understandability', 'Easy to Understand', 'Lack of Confusion', 'Unambiguity', 'Concise', 'Readability', 'Interpretability']", "['Customer Needs and Satisfaction', 'Market Penetration']", "Clarity in data refers to how easily it can be understood and interpreted, emphasizing simplicity, structure, consistency, readability, contextual explanation, appropriate visualization, and completeness to ensure users can draw accurate insights without confusion.", "Clarity in the context of data refers to how easily the data can be understood, interpreted, and used by its intended audience. It involves presenting data in a way that is straightforward, unambiguous, and free from unnecessary complexity. Clear data allows users to quickly grasp its meaning, draw insights, and make decisions without confusion or misinterpretation. The key aspects of data clarity from a KPI perspective are:\nSimplicity and Structure: Data should be presented in a well-organized manner, often through tables, graphs, or summaries that make complex data easier to digest. Example: Using clear, concise headers in a dataset and avoiding overly complex structures.\nConsistency: Consistent terminology, formats, and units of measurement improve clarity. Data should follow the same structure and presentation style across the dataset.\nReadability: Data should be easily readable, with clear labels, appropriate formatting, and a clean layout. Good readability ensures that the user can navigate the data without confusion.\nContext, Metadata, and/or Explanation: Providing context or explanations (e.g., metadata, labels, or definitions) that clarify what the data represents.\nRedundancy/Containment Fraction: Data should be free from unnecessary repetition or irrelevant (Within the same dataset only, since redundancy is important for availability) information that can clutter the understanding. Example: Avoiding the use of the same information in multiple columns or rows when that doesn't add value to an analysis.\nVisualization: Using the right type of visualization (charts, graphs, maps, etc.) can enhance clarity by making patterns, trends, or insights easier to recognize.\nCompleteness: checks if the dataset has all the required fields and values without missing information (measured as a percentage).\nNot all these factors are included in the main taxonomy figure, for simplicity they have been reduced there. Nevertheless, these factors could be considered as a base since several of the main components here can be bound to readability, conciseness, and understandability."
CO2 Savings, "[]", "['Innovation and Growth']", "CO2 Savings measures the reduction in data center carbon emissions from a baseline, reflecting the impact of energy optimization, renewable integration, and heat recovery strategies like those implemented in the GENiC system.", "The metric CO2 Savings refers to the change in data center CO2 emissions from a given baseline. This metric evaluates the reduction in CO2 emissions achieved by implementing the GENiC system and its integrated management strategies for optimizing energy usage, renewable energy integration, and heat recovery in data centers."
Completeness, "['Appropriate Amount of Data']", "['Data Quality']", "Completeness in data refers to the extent to which all necessary and expected information is present in a dataset, ensuring mandatory fields are filled, missing values are minimal, and the data is sufficiently detailed for accurate analysis.", "Completeness in data refers to the extent to which all required data is available and present within a dataset. It measures how much of the expected information is provided, indicating whether or not critical data fields are missing or left blank.\nKey aspects of data completeness include:\nPresence of mandatory fields: All required fields have values.\nProportion of missing data: A low percentage of missing or null values indicates better completeness.\nConsistency with expectations: The dataset should align with the expected structure and content, covering all data points.\nGranularity: The data should provide enough detail to meet the desired level of analysis.\nFor example, in a customer database, completeness would ensure that fields like customer names, addresses, and contact details are filled for every record. A lack of data in important fields would indicate incomplete data."
Regulatory Compliance, "['Compliance Cost']", "['Data Governance and Compliance']", "Compliance measures the extent to which data adheres to legal, regulatory, and organizational standards, assessing both the alignment with required rules and the potential financial risks of non-compliance.", "Regulatory Compliance measures whether the data complies with relevant legal and regulatory standards. Estimated financial cost to business of not keeping data for compliance/regulation purposes. It can be defined as a quantitative or qualitative measure that evaluates the extent to which data adheres to the predefined rules, standards, or requirements. Compliance metrics focus on ensuring data consistency, reliability, and alignment with organizational or regulatory frameworks."
Conciseness, "['Concise Representation', 'Simplicity']", "['Data Quality']", "Conciseness in data refers to presenting essential information clearly and efficiently, minimizing redundancy and irrelevant details while maintaining clarity, relevance, and semantic consistency for effective communication and understanding.", "Conciseness refers to the principle of presenting data or information in a way that is both brief and clear, without unnecessary detail or redundancy. The goal is to convey the essential meaning or facts in the most straightforward and efficient manner possible, while still being complete and accurate. Key Characteristics of Concise Representation could include (To chose from the tyep of system considerations):\nBrevity/Compression: The information is presented using the fewest words, symbols, or data points necessary to communicate the intended meaning. It also Reflects how much data can be compacted without losing critical information. \nClarity: The content is clear and easy to follow, even when reduced to its essential elements. \nEfficiency/Signal-to-noise Ratio: It avoids redundancy, long-winded explanations, or superfluous data that could confuse or overwhelm the user. Also, can be considered as the proportion of useful, relevant information (signal) to irrelevant or extraneous data (noise). \nRedundancy: Measures the repetition of identical or similar data within a system or dataset.\nRelevance: Only the most relevant information is included, eliminating any extraneous data that doesn't add value to the understanding of the content.\nSemantic Consistency: Measures alignment and consistency in the terminology and format used across the dataset.\nDuplicate Elimination: a measure for the process of identifying and removing identical or near-identical records."
Confidence, "[]", "['Operational Efficiency', 'Data Quality']", "TODO", "The confidence metric measures how strongly a dependency (MD or MFD TODO: unclear what these are.) holds in the dataset."
Consistency, "['Heterogeneity', 'Veracity']", "['Data Quality']", "Consistency in data refers to the uniformity, reliability, and rule-conformance of data across systems, ensuring accuracy, coherence, and integrity by validating against predefined rules and maintaining synchronization across datasets.", "Consistency refers to how well data values conform to predefined rules, such as association rules in the context of relational databases. The metric for consistency, as defined by Alpar and Winkelsträter [118], evaluates the consistency of a tuple t based on whether it fulfills or violates certain rules r from a set R of association rules. Consistency, in the context of data management and information systems, refers to the uniformity and reliability of data across different datasets, systems, and applications. It ensures that data remains accurate, coherent, and free from contradictions, which is essential for maintaining data integrity and trustworthiness. Key aspects of consistency (for a KPI consideration) include: Data Integrity, Uniformity, Synchronization, Rules validation, Error Prevention, and Governance."
Containment Fraction, "[]", "['Operational Efficiency', 'Data Quality']", "Measures the extent to which one dataset is contained within another.", "Measures the extent to which one dataset is contained within another. This metric helps identify redundancy and optimize storage usage. A limit in containment fraction has to exist since availability depends on duplication, for security, of the same information."
Cooling Effectiveness Rate, "['CER', 'Energy Effectiveness of Cooling']", "['Operational Efficiency']", "Focuses on the effectiveness of cooling systems in the data center.", "Focuses on the effectiveness of cooling systems in the data center. There are direct linkage to other metrics related to cooling effectivenes, such as ”Energy Effectiveness of Cooling Mode in a Season”. Assesses how effectively the cooling system operates in different seasons, which can influence operational costs and carbon emissions."
Cost of Degradation, "['CoD']", "['Data Quality']", "Cost of Degradation measures the reduction in data quality caused by transformations, particularly in privacy-preserving contexts. It helps quantify how much value the data has lost through necessary adjustments for privacy.", "Cost of Degradation measures the reduction in data quality caused by transformations, particularly in privacy-preserving contexts. It helps quantify how much value the data has lost through necessary adjustments for privacy."
Data Acquisition Cost, "['DAC']", "['Data Monetization']", "Data Acquisition Cost (DAC) represents the proportion of data or local models that a Data Acquirer must obtain from Data Providers during each federated learning training epoch to build an accurate global model, serving as a non-monetary measure of acquisition effort.", "Data Acquisition Cost (DAC) refers to the cost, typically measured as a percentage, associated with acquiring local models from the Data Providers (DPs) during each training epoch in a federated learning setup. The DAC represents the fraction of data or local models the Data Acquirer (DA) needs to purchase or gather from the DPs to train an accurate and effective global model. In the traditional sense, DAC is not directly a monetary cost; rather, it represents the fraction of available data or local models that must be procured. All other costs are encapsulated within the metric Cost (e.g. Purchase Cost, OPEX, etc.)."
Data Centre Adaptation, "['Data Centre Energy Profile Change']", "['Operational Efficiency']", "The Data Centre Adaptation (DCA) metric measures the change in a data center's energy profile from a predefined baseline, reflecting its adaptability, energy efficiency, and integration of renewable energy within the GENiC framework.", "The Data Centre Energy Profile Change is referred to as DCA (Data Centre Adaptation) in the context of energy metrics outlined in the GENiC framework from the manuscript. It captures the change in the data center's energy profile from a predefined baseline. This metric is part of a broader set of evaluation measures used to assess energy efficiency, sustainability, and the ability to adapt to renewable energy integration [44]."
Data Ingestion Capabilities, "['Data Collection and Management Capabilities', 'Data Analysis', 'Data Mining', 'Data Sources']", "['Technology and Infrastructure']", "Data Ingestion Capabilities refer to a system's ability to efficiently collect, process, and store large volumes of data from diverse sources in real time, ensuring high throughput, low latency, scalability, and data integrity, especially in high-velocity environments like IoT.", "Data Ingestion Capabilities refer to the ability of a system to effectively and efficiently handle the process of acquiring, collecting, and storing data from various sources. This capability is critical for managing large-scale, high-velocity data flows, especially in contexts like IoT applications, where data is generated continuously by sensors and devices. Effective data ingestion ensures the system's ability to (key components): Handle High Throughput (Throughput), Accommodate Delays (Latency, and handling delayed data), Ensure Scalability (e.g. Scalability is evaluated by testing throughput and latency as the number of devices, sensors, or data points increases), Maintain Data Integrity. In practice, this involves technologies like time-series databases, message queues (e.g., Kafka), and specialized data formats like TsFile for compression and organization."
Data Price, "['Data Value', 'Price Function', 'Payoff Reimbursement', 'Financial Value', 'Price of Information']", "['Data Monetization']", "Data price refers to the monetary value assigned to data, determined through models or market mechanisms, incorporating factors like cost, contribution rewards, compensation, and volume-based pricing using approaches such as entropy valuation, utility models, or dynamic pricing.", "Data price is the financial value assigned to data, determined through models or market mechanisms. It encompasses concepts like monetary cost, price functions, payoff (reward for contributions), reimbursement (compensation for costs), and volume-based metrics (e.g., price per MB). Methodologies include entropy-based valuation, utility models, and dynamic pricing, tailored to specific contexts such as privacy, real-time trading, or structured markets."
Oversight, "['Audit']", "['Data Governance and Compliance']", "Governance Audit Capability assesses an organization's ability to monitor, validate, and ensure adherence to governance standards through mechanisms like periodic audits and compliance checks.", "Assesses the organization's capability to monitor and validate compliance with governance standards, processes, and policies. It includes mechanisms for periodic audits to ensure data integrity and proper governance implementation."
Data Principles and Practices, "['Data-Standard Driven', 'Standardization']", "['Data Governance and Compliance']", "Data Quality Compliance measures the extent to which an organization adheres to defined data standards and principles, reflecting its commitment to ensuring data consistency, reliability, and alignment with organizational goals through quantifiable metrics.", "This metric measures the degree to which an organization adheres to defined data standards and principles to maintain data quality and governance. It emphasizes commitment to data quality policies, ensuring data is consistent, reliable, and aligned with organizational objectives [97], [57], [54].\nKey components:\nCommitment to Data Quality: Demonstrates adherence to predefined data standards and a commitment to maintaining high-quality data across systems and processes [57], [54].\nQuantifiable Standards: Measures compliance through clear metrics, such as achieving specific scores or thresholds in data quality assessments [57], [54]."
Data Similarity, "['Euclidian Distance', 'Projection Similarity', 'Similarity Score', 'Cosine Similarity', 'Average Distance', 'Kolmogorov-Smirnov (KS)', 'Mann-Whitney (MW)', 'Mood s Median (MD)', 'Levene (LE)']", "['Data Quality']", "Data Similarity refers to the numerical measurement of how closely related two data points or datasets are, typically using distance or similarity metrics like Euclidean Distance or Cosine Similarity, and is distinct from syntactic similarity due to its focus on numerical attributes.", "These metrics are a measure to compute the distance between two points within a given space, commonly applied to numerical attributes. Such metrics are distinguished from syntactic similarity due to their numerical basis. Projection Similarity, for instance, calculates the similarity of datasets based on feature dimensions from other datasets."
Data Type, "[]", "['Data Valuation Techniques']", "Data can be classified into categories (A–D) based on its intended use, ranging from time-sensitive operational data to long-term legal, decision-making, or research-related data, each with distinct characteristics in terms of frequency, lifespan, value realization, and sensitivity.", "Depending on the possible use of the data, it can be classified. In [117], is classified or scored on the decision-based valuation method. (A, B, C, and D) A is operational data (i.e. It has a predetermined frequency; It has a short usable life span proportional to its frequency; It is very time sensitive and its value depreciates rapidly after its timeframe; and Its value is quickly realized through its use.) B is a One Time Decision Data (i.e. It does not have a predetermined frequency; It has a usable life span of the length of the decision; It is often not very time sensitive; There is a high emphasis on accuracy; It has a high value but often a high cost as well; and Its value takes a long period of time to be realized.) Type C is Legal and Safety Data (i.e. It has almost no value to the organization; The true value of the data can only be estimated on the probability of its need; It often has to be stored for a minimum of five years if not longer; The organization is legally required to collect and store it; There can often be a legally required frequency; and It can have an indefinite life span.) and D is Research and Innovation data (i.e. It has a very low chance of providing future incomes; It is difficult to predict whether it will provide value; It has a large value range; and It has a long life span). TODO: This explanation is horrendous, need to find a better way to make this clear."
Data Value, "['DV', 'Data Criticallity', 'Value of Information', 'Fixed Record Value', 'IP Value', 'Intrinsic Record Value']", "['Data Valuation Techniques']", "Data value refers to the level of usefulness or relevance data holds for a specific consumer or context, varying with application needs, and is often assessed through factors like cost, lifecycle, and criticality to business operations and decision-making.", "Level of usefulness or relevance a data object has for a particular data consumer, which can vary depending on the consumer's application context and specific needs. This means that Data Value is not static; it changes based on how well the data fits the purpose it's being used for, even if the Data Quality (DQ) of the data remains constant. For distribucion services it can measures the value of a decision node in the Decision-Based Valuation (DBV) framework, taking into account factors like cost and lifecycle. Also, can refer to Data Criticality: refers to the importance of data for both consumers and the company. It evaluates how essential certain data is in the context of business operations and decision-making. The measure of data criticality assesses the impact that data has on the organization and its customers, particularly regarding its role in supporting business processes, revenue generation, and service delivery."
Data Value Ratio, "['VR']", "['Data Valuation Techniques']", "The Data Value Ratio (VR) represents the proportion of a decision node's value attributed to a specific data source.", "The Data Value Ratio (VR) represents the proportion of a decision node's value attributed to a specific data source."
Demand, "['Score', 'Number of Data Consumers']", "['Data Monetization']", "Data demand refers to the desirability or market need for specific data types, typically measured through buyer willingness-to-pay scores or strategic value within a broader data value chain, often informing pricing and data management strategies.", "The demand or score for data is defined differently depending on the system under consideration and thus, the strategy to be implemented. For example, in the [TODO: Broken reference], demand for data is conceptualized as buyers' willingness to pay for specific types of information. This demand is measured using a scoring mechanism where buyers rate the desirability of different categories of data. The resulting score reflects the demand for that data type in the market. Combined with the data's utility (a measure of how much the data quality degrades after privacy-preserving obfuscation), these scores are used to establish a pricing model for information. In [TODO: Broken reference], demand or value of data is analyzed as part of a broader data value chain management. Here, demand is less about individual buyers and more about the data's role in enabling strategic decisions, cost efficiency, and economic outcomes."
Detail, "[]", "['Data Quality']", "Data Detail refers to the level of specificity and descriptive richness in a dataset, typically measured through granularity, number of descriptive attributes, and the precision with which data points are captured.", "There is no clear specification of Detail for its estimation in the literature. Nevertheless, purely technical accuracy consideration could imply how finely data is broken down or how much descriptive information is captured in each data point. Then Key Aspects of Data incorporate are:\nGranularity: The extent to which data is broken down into smaller, specific components. High granularity means more detailed data.\nDescriptive Attributes: The number of attributes or fields that describe an entity or event. More detailed datasets have a larger number of attributes that provide rich, descriptive context.\nPrecision: How exact the data is. For numerical data, this might refer to how many decimal points are included, for temporal data, the description of timestamps, while for categorical data, it refers to the specificity of categories."
Differential Privacy, "[]", "['Data Governance and Compliance']", "Same as Inferential Privacy, but in the opposite direction (Check for Inferential Privacy).", "Same as Inferential Privacy, but in the opposite direction (Check for Inferential Privacy). Also as defined in [TODO: Broken reference], a metric used to gauge the level of privacy preserved in data when shared or sold in marketplaces. Also, The(ϵ, δ) -differential privacy is a widely adopted concept to quantify the privacy loss in private machine learning (ML) algorithms."
Discoverability, "['Effectiveness (Platform), 'Platform Performance']", "['Innovation and Growth']", "Discoverability measures how easily a dataset or platform allows users to find relevant information, focusing on the effectiveness of search functionality and the dataset’s ability to meet users’ informational needs.", "As defined in [34], this is a metric that describes the ability of a dataset to show the content of information and be able to search or discover datasets. For a platform, measures the platform's ability to meet the user's information needs accurately. This KPI evaluates how effective the search functionality is in returning relevant datasets based on user input."
Space Cost, "['Disk Occupation']", "['Data Valuation Techniques']", "Space Cost refers to the amount of storage required to retain data, influenced by file format and storage mechanisms, and can be tied to the ongoing costs of maintaining that data.", "Space Cost refers to the amount of storage space required to store data, which can vary depending on the file format and storage mechanisms used. The cost can be linked later on the cost associated to maintain such information."
Downloads, "['Download Frequency']", "['Customer Needs and Satisfaction', 'Market Penetration']", "The number of times users have dowloaded a dataset.", "The number of times users have dowloaded a dataset."
Economic Efficiency, "['Data Business Characteric Index', 'Business Criticality']", "['Data Monetization']", "Economic Efficiency / Business Criticality is an index that evaluates the importance of data to business operations by weighting its relevance to key processes, while also ensuring that the application of data quality measures yields benefits that justify their associated costs.", "This index represents how critical the data is for business processes. It is determined by assigning weights to data based on its relevance to various operational requirements, with higher-weighted data indicating greater business importance. Economic efficiency can also refers to the cost-benefit balance when applying data quality metrics (i.e. another operation). The requirement of economic efficiency, labeled as 'Requirement', ensures that the use of data quality metrics must justify the costs involved, meaning the benefits obtained from improved data quality should outweigh the costs of measuring and improving that quality."
Encryption Time, "['ET', 'Decryption Time', 'DT']", "['Operational Efficiency']", "Measures the time taken to convert plaintext data into an encrypted format using encryption algorithms.", "This metric measures the time taken to convert plaintext data into an encrypted format using encryption algorithms. It's important in cloud and data security to understand how quickly data can be secured before transmission or storage."
Energy Efficiency, "['Power to Performance Effectiveness', 'PPE', 'Power Usage Effectiveness', 'PUE', 'Data Centre Infrastructure Efficiency', 'DCiE', 'Data Centre Performance per Energy', 'DPPE', 'Data Centre Energy Productivity', 'DCEP', 'Communication Network Energy Efficiency', 'CNEE', 'Network Power Usage Effectiveness', 'NPUE', 'Energy Proportionality Coefficient', 'EPC']", "['Operational Efficiency']", "Energy Efficiency Metrics for Data Centers assess how effectively energy is utilized relative to performance outputs, combining ratios like PUE, DCiE, DCEP, PPE, CNEE, NPUE, and EPC to provide a comprehensive view of how well IT equipment, networks, and infrastructure convert power into useful computational work.", "Measures how efficiently the system handles workloads concerning energy consumption. This ratio is a key metric for operational efficiency. The PUE Measures the energy efficiency of a data center by comparing the total facility energy to the energy used by IT equipment. The DCiE is the inverse of PUE, used to measure the ratio of IT equipment energy to the total facility energy. Developed by Japan's Green IT Promotion Council, the Data Cetnre Perfromance per Energy metric combines four sub-metrics: Green Energy Consumption (GEC), Power Usage Effectiveness (PUE), IT Equipment Energy Efficiency (ITEE), and IT Equipment Utilization (ITEU). It provides a holistic view of the energy efficiency of data centers, factoring in both facility and IT equipment performance. Similarly, Power to Performance Effectiveness (PPE) is a metric that evaluates the efficiency of energy consumption in relation to the performance output of IT equipment within a data center. Specifically, PPE measures how much power is consumed to achieve a certain level of performance. DCEP: This metric measures the productivity of a data center relative to the energy it consumes. It evaluates how much useful work (e.g., computational tasks, data processing) is accomplished per unit of energy consumed. Communication Network Energy Efficiency (CNEE) is a metric that evaluates the energy efficiency of a data center's communication network. It measures the amount of energy required to deliver a unit of data (usually a bit or byte) across the network within a data center. NPUE: This is a more specific metric that focuses on the energy consumed by the networking components of the data center. EPC: evaluates how well the energy consumption of individual devices or systems (such as servers, networking devices, or storage systems) scales with their workload. In an ideal scenario, a system with high EPC would use energy proportionally to its workload, consuming minimal energy when under low or no load."
Power Usage Efficiency, "['Total Energy Usage Effectiveness', 'TUE', 'Data Center Efficiency', 'DCE']", "['Operational Efficiency']", "TODO", "TODO: WHAT IS THE DIFFERENCE WITH THE PREVIOUS ONE?!"
Total Equipment Utilization, "['TEU']", "['Operational Efficiency']", "TODO", "TODO: needs more explation. Encompasses the overall facility's resource usage. It includes cooling, power distribution, and other supporting infrastructure components. "
IT Equipment Utilization, "['ITEU']", "['Operational Efficiency']", "TODO", "The ITEU metric focuses on the ratio of actual energy consumption to the rated energy consumption of IT equipment, providing insight into the energy utilization efficiency of the IT infrastructure. TODO: maybe more detail?"
IT Equipment Energy Efficiency, "['ITEE']", "['Operational Efficiency']", "TODO", "TODO: missing explanation here, same as the previous one."
Data Center Performance Per Energy, "['DPPE']", "['Operational Efficiency']", "TODO", "TODO: Again, seems to be already included in Energy Efficiency."
Energy Reuse Fraction, "['ERF', 'Energy Reuse Effectiveness', 'ERE']", "['Operational Efficiency']", "The ERF, defined in ISO/IEC 30134-6 / EN 50600-4-6, determines the share of the total energy consumption that is reused.", "The ERF, defined in ISO/IEC 30134-6 / EN 50600-4-6, determines the share of the total energy consumption that is reused."